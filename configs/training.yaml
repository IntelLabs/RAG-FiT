model:
    _target_: ragfit.models.hf.HFTrain
    model_name_or_path: microsoft/Phi-3-mini-128k-instruct
    load_in_4bit: false
    load_in_8bit: true
    torch_dtype:
    device_map:
    trust_remote_code: true
    lora:
        bias: none
        fan_in_fan_out: false
        layers_pattern:
        layers_to_transform:
        lora_alpha: 16
        lora_dropout: 0.1
        peft_type: LORA
        r: 16
        target_modules:
            - qkv_proj
        task_type: CAUSAL_LM
        use_rslora: true
    completion_start: <|assistant|>
    instruction_in_prompt:
    max_sequence_len: 4000

train:
    output_dir: ./trained_models
    bf16: false
    fp16: false
    gradient_accumulation_steps: 4
    group_by_length:
    learning_rate: 2e-5
    logging_steps: 10
    lr_scheduler_type: cosine
    max_steps: -1
    num_train_epochs: 1
    per_device_train_batch_size: 1
    optim: adamw_torch_fused
    remove_unused_columns: true
    save_steps: 20000
    save_total_limit: 2
    warmup_ratio: 0.03
    weight_decay: 0.001
    report_to: wandb

instruction: ragfit/processing/prompts/prompt_instructions/qa.txt
template:                       # specify a template file or use chatML format with tokenizer's chat template
data_file: my-processed-data.jsonl
input_key: my_prompt
output_key: answer
resume_checkpoint:
limit:
shuffle:
use_wandb:
hfhub_tag:
experiment: phi-training
wandb_entity:
