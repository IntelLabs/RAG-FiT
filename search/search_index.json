{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>RAG-FiT is a library designed to improve LLMs ability to use external information by fine-tuning models on specially created RAG-augmented datasets. The library helps create the data for training, given a RAG technique, helps easily train models using parameter-efficient finetuning (PEFT), and finally can help users measure the improved performance using various, RAG-specific metrics. The library is modular, workflows are customizable using configuration files. Formerly called RAG Foundry.</p> <p>Comments, suggestions, issues and pull-requests are welcomed! \u2764\ufe0f</p>"},{"location":"#installation","title":"Installation","text":"<p>Clone and run:</p> <pre><code>pip install -e .\n</code></pre> <p>Optional packages can be installed: <pre><code>pip install -e .[haystack]\npip install -e .[deepeval]\n</code></pre></p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>For a simple, end-to-end example, see the PubmedQA Tutorial.</p>"},{"location":"#overview","title":"Overview","text":"<p>The RAG-FiT framework facilitates fast prototyping and experimentation with various RAG settings and configurations, including data selection and filtering, processing, retrieval, ranking, query manipulation, prompt generation, training, inference, output processing and evaluation. The library is comprised of 4 modules: dataset creation, training, inference and evaluation.</p> <ul> <li> <p>Dataset Creation: The processing module creates datasets, persisting RAG interactions, to be used for RAG training and inference. RAG interactions include dataset loading, columns normalization, data aggregation (fewshot creation), information retrieval using external tools and frameworks, API integration, template-based prompt creation and any other form of pre-processing. The data is saved in a consistent, model-independent, input-output format, along with all other fields and metadata. See Processing.</p> </li> <li> <p>Training: using PEFT for efficient training and TRL (e.g. supervised FT) users can train any model on the augmented datasets. Training is done on the completions. Models can be pushed to HF Hub. See Training.</p> </li> <li> <p>Inference: generating predictions using the augmented datasets with trained or untrained LLMs. See Inference.</p> </li> <li> <p>Evaluation: running evaluation on the generated output from the inference module. Users can provide a list of metrics to run; custom metrics can be implemented easily. Current metrics include EM, F1, ROUGE, BERTScore, Deepeval, RAGAS, HF <code>evaluate</code> and classification. Metrics can be local\u2014run on each example, or global\u2014run on the entire dataset, e.g. recall. Metrics can utilize any feature in the dataset, like retrieval results, reasoning, citations and attributions, not just the input and output texts. See Evaluation.</p> </li> </ul>"},{"location":"#running","title":"Running","text":"<p>The 4 modules are represented as scripts: <code>processing.py</code>, <code>training.py</code>, <code>inference.py</code> and <code>evaluation.py</code> at the top level. Every call has the form <code>python SCRIPT options...</code>.</p> <p>The library utilizes the Hydra configuration tool; it enables the use of hierarchical configurations, easily overridden of values in the CLI and the ability to run multiple jobs remotely (e.g. integrations with SLURM and Ray). It represents a configuration-as-code approach, as it can instantiate python classes according to configuration (the <code>_target_</code> keyword indicates the python class to use in a given context).</p> <p>There are default configurations for each module in the configs folder. A configuration file can be overridden like so:</p> <pre><code>python processing -cp configs/paper -cn processing-asqa-retrieval\n</code></pre> <p>Individual keywords can be overridden as well: <pre><code>python processing -cp configs/paper -cn processing-asqa-retrieval   \\\n       output_path=/store/data/here                                 \\\n       cache=true\n</code></pre></p> <p>For a complete set of configurations, reproducing the experimentation in the paper with the ASQA dataset, see the configurations in the Paper folder.</p>"},{"location":"#citation","title":"Citation","text":"<p>Please cite our paper if it helps your research: RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation.</p> <pre><code>@article{fleischerRAGFoundryFramework2024,\n  title =        {{RAG} {Foundry}: {A} {Framework} for {Enhancing} {LLMs} for {Retrieval} {Augmented} {Generation}},\n  author =       {Fleischer, Daniel and Berchansky, Moshe and Wasserblat, Moshe and Izsak, Peter},\n  year =         2024,\n  note =         {arXiv:2408.02545 [cs]},\n  annote =       {Comment: 10 pages},\n  url =          {http://arxiv.org/abs/2408.02545},\n  publisher =    {arXiv},\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>The code is licensed under the Apache 2.0 License.</p>"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>This is not an official Intel product.</p>"},{"location":"evaluation/","title":"Evaluations","text":"<p>The evaluation module takes the produced inference file and the original processed dataset and runs a list of evaluations, producing a final results file, in a YAML format. The evaluations are represented as metric classes.</p> <p>We implement several metrics including: a wrapper for HuggingFace <code>evaluate</code> class, which can accept a list of metrics, EM, F1, classification (accuracy, precision, recall, F1), BERTScore, Semantic similarity (using a customizable cross-encoder). The module can also run metrics from DeepEval, which offers a large collection of LLM evaluations.</p> <p>Metrics can be either local or global; a local metric runs over each example individually, scores are collected and averaged. A global metric runs on the entire dataset at once, for example: classification F1.</p> <p>The configuration contains the following section:</p> <p><pre><code>answer_processor:\n  _target_: ragfit.processing.answer_processors.regex.RegexAnswer\n  capture_pattern:          # \"&lt;ANSWER&gt;: (.*)\"\n  stopping_pattern:         # \"[,.;]\"\n</code></pre> The evaluation module introduces the concept of an Answer Processor. This class can run post-processing on the generated text, preparing it for evaluations or the specific format some metrics require.</p> <p>There is a default processor, called <code>RegexAnswer</code>; it can filter text, based on a python regex capture pattern. It can also split text using a stopping pattern. For example, in the Chain-of-Thought reasoning we used in the paper, the model is instruction to explain its answer, cite if needed and finally print the final results in the following format <code>&lt;ANSWER&gt;: ...</code>. We can use this format as a capture pattern; thus models that learn to answer using this pattern (obey the instruction) will score higher.</p> <p>Next is a list of metrics; each one is a python class: <pre><code>metrics:\n  - _target_: ragfit.evaluation.metrics.HFEvaluate\n    metric_names: [rouge]\n  - _target_: ragfit.evaluation.metrics.EM\n  - _target_: ragfit.evaluation.metrics.F1\n  - _target_: ragfit.evaluation.metrics.BERTScore\n    model: microsoft/deberta-large-mnli\n</code></pre></p> <p>Some metrics require additional parameters, for example HuggingFace <code>evaluate</code> requires the metrics' names, BERTScore requires an embedding model.</p> <p><pre><code>key_names:\n  generated: generated\n  label: answer\n  query: query\n  context: context\n</code></pre> A mapping of keys and values: the values should represent the names of the corresponding fields in the processed dataset.</p> <p>Finally: <pre><code>results_file: my-evaluation.yaml\ngenerated_file: inference.jsonl\ndata_file: my-processed-data.jsonl\nlimit:\n</code></pre></p> <p>One needs to provide the generated inference file, the processed dataset and a filename for the results summary. A limit number of rows can be provided for debugging purposes.</p>"},{"location":"evaluation/#running-evaluations-on-asqa","title":"Running Evaluations on ASQA","text":"<p>As the final part of the demonstration of the framework with the ASQA dataset and Phi-3 models, we will evaluate the different RAG configurations, with and without the use of fine-tuning.</p> <p>As a reminder, ASQA has 2 types of answers: long answer and short answers. We will evaluate the generated answers using the long answer with RAGAS metrics (faithfulness and relevancy) and use the short answers with ASQA defined STR-EM.</p>"},{"location":"evaluation/#short","title":"Short","text":"<p>Starting with the short answers, the label keyword is <code>answer-short</code> (recall the processing) and a representative configuration looks like this:</p> <pre><code>answer_processor:\n    _target_: ragfit.processing.answer_processors.regex.RegexAnswer\n    capture_pattern: \"&lt;ANSWER&gt;: (.*)\"\n    stopping_pattern:\n\nmetrics:\n    - _target_: ragfit.evaluation.metrics.StringEM\n\nkey_names:\n    generated: text\n    label: answer-short\n    query: query\n\nresults_file: evaluation-asqa-baseline.yaml\ngenerated_file: asqa-baseline-dev-generated.jsonl\ndata_file: asqa-baseline-dev.jsonl\n</code></pre> <p>Here are the calls to evaluate the different configurations:</p> <p>Baseline: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-short\n</code></pre></p> <p>Context: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-short    \\\n       results_file=asqa-context-dev-generated-results.yaml         \\\n       data_file=asqa-context-dev.jsonl                             \\\n       generated_file=asqa-context-dev-generated.jsonl\n</code></pre></p> <p>Context with fine-tuned model: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-short    \\\n       results_file=asqa-context-ft-dev-generated-results.yaml      \\\n       data_file=asqa-context-dev.jsonl                             \\\n       generated_file=asqa-context-ft-dev-generated.jsonl\n</code></pre></p> <p>Chain-of-Thought: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-short    \\\n       results_file=asqa-cot-dev-generated-results.yaml             \\\n       data_file=asqa-cot-dev.jsonl                                 \\\n       generated_file=asqa-cot-dev-generated.jsonl\n</code></pre></p> <p>Chain-of-Thought with fine-tuned model: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-short    \\\n       results_file=asqa-cot-ft-dev-generated-results.yaml          \\\n       data_file=asqa-cot-dev.jsonl                                 \\\n       generated_file=asqa-cot-ft-dev-generated.jsonl\n</code></pre></p>"},{"location":"evaluation/#long","title":"Long","text":"<p>Evaluation the generated output with respect to the full answer, we use two RAGAS metrics, namely faithfulness and relevancy. The RAGAS metrics require a context for the critic to make a judgment, so these are not relevant for the baseline configuration.</p> <p>The different in configuration is in the list of metrics and keywords:</p> <pre><code>metrics:\n    - _target_: ragfit.evaluation.deep.Faithfulness\n      azure_endpoint: azure.endpoint.com\n      azure_deployment: GPT-4-32k-Bot\n      api_version: 2024-05-01-preview\n    - _target_: ragfit.evaluation.deep.Relevancy\n      azure_endpoint: azure.endpoint.com\n      azure_deployment: GPT-4-32k-Bot\n      api_version: 2024-05-01-preview\n      embeddings: BAAI/bge-small-en-v1.5\n\nkey_names:\n    generated: text\n    label: answers\n    query: query\n    context: positive_passages\n</code></pre> <p>The relevancy metrics an embedder\u2014it generates probable questions based on the generated answer (and the context) and then measures semantic similarity to the original question.</p> <p>Context: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-long     \\\n       results_file=asqa-context-dev-generated-results-ragas.yaml   \\\n       data_file=asqa-context-dev.jsonl                             \\\n       generated_file=asqa-context-dev-generated.jsonl\n</code></pre></p> <p>Context with fine-tuned model: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-long         \\\n       results_file=asqa-context-ft-dev-generated-results-ragas.yaml    \\\n       data_file=asqa-context-dev.jsonl                                 \\\n       generated_file=asqa-context-ft-dev-generated.jsonl\n</code></pre></p> <p>Chain-of-Thought: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-long \\\n       results_file=asqa-cot-dev-generated-results-ragas.yaml   \\\n       data_file=asqa-cot-dev.jsonl                             \\\n       generated_file=asqa-cot-dev-generated.jsonl\n</code></pre></p> <p>Chain-of-Thought with fine-tuned model: <pre><code>python evaluation.py -cp configs/paper -cn evaluation-asqa-long     \\\n       results_file=asqa-cot-ft-dev-generated-results-ragas.yaml    \\\n       data_file=asqa-cot-dev.jsonl                                 \\\n       generated_file=asqa-cot-ft-dev-generated.jsonl\n</code></pre></p>"},{"location":"inference/","title":"Inference","text":"<p>In the inference stage, we take the processed dataset and LLM and make predictions. The LLM can be fine-tuned. The processed data encapsulates the RAG interactions: pre-processing, retrieval, ranking, prompt-creation, and possibly other types of transformations. So this step deals with producing the predictions to be evaluated.</p> <p>It is simple in nature, described by the following configuration:</p> <pre><code>model:\n    _target_: ragfit.models.hf.HFInference\n    model_name_or_path: microsoft/Phi-3-mini-128k-instruct\n    load_in_4bit: false\n    load_in_8bit: true\n    device_map: auto\n    torch_dtype:\n    trust_remote_code: true\n    instruction: ragfit/processing/prompts/prompt_instructions/qa.txt\n    instruct_in_prompt: false\n    lora_path:\n    generation:\n        do_sample: false\n        max_new_tokens: 50\n        max_length:\n        temperature:\n        top_k:\n        top_p:\n        return_full_text: false\n\ndata_file: asqa-baseline-dev.jsonl\ngenerated_file: asqa-baseline-dev-generated.jsonl\ninput_key: prompt\ngeneration_key: output\ntarget_key: answers\nlimit:\n</code></pre> <p>The model section deals with details regarding the model loading and generation options. System instruction can be provided, as we mentioned previously: the datasets are model independent, and all model details (system instruction, custom chat template) are needed only during training and inference. Similarly, <code>instruct_in_prompt</code> inserts the system instruction inside the prompt, for models which don't support a system role.</p> <p>Other parameters: - Data file is the processed file. - Generated file is the file that will be created with the completions (and labels, for easy debugging). - Target key is the label keyword. - Limit: to a number of examples, for debugging.</p>"},{"location":"inference/#running-inference","title":"Running Inference","text":"<p>In order to run evaluations for ASQA, like in the paper, there are 5 configurations to run: baseline, context, context with fine-tuned model, CoT reasoning, and CoT reasoning with a model that was fine-tuned with distractor documents.</p> <p>The baseline inference uses the configuration as is; the other calls, use the configuration and just override the value of the processed data to use and optionally LORA path for the model.</p> <p>Baseline: <pre><code>python inference.py -cp configs/paper -cn inference-asqa\n</code></pre></p> <p>Context: <pre><code>python inference.py -cp configs/paper -cn inference-asqa    \\\n       data_file=asqa-context-dev.jsonl                     \\\n       generated_file=asqa-context-dev-generated.jsonl\n</code></pre></p> <p>Context with fine-tuned model: <pre><code>python inference.py -cp configs/paper -cn inference-asqa    \\\n       data_file=asqa-context-dev.jsonl                     \\\n       generated_file=asqa-context-ft-dev-generated.jsonl   \\\n       model.lora_path=./path/to/lora/checkpoint\n</code></pre></p> <p>Chain-of-Thought: <pre><code>python inference.py -cp configs/paper -cn inference-asqa    \\\n       data_file=asqa-cot-dev.jsonl                         \\\n       generated_file=asqa-cot-ft-dev-generated.jsonl\n</code></pre></p> <p>Chain-of-Thought with fine-tuned model: <pre><code>python inference.py -cp configs/paper -cn inference-asqa    \\\n       data_file=asqa-cot-dev.jsonl                         \\\n       generated_file=asqa-cot-ft-dev-generated.jsonl       \\\n       model.lora_path=./path/to/lora/checkpoint\n</code></pre></p>"},{"location":"inference/#running-inference-with-vllm-backend","title":"Running Inference with vLLM Backend","text":"<p>To achieve potentially faster inference speeds, you can run inference using the vLLM backend. The functionality of the inference process remains similar to the previously defined process, with the addition of extra arguments that can be used with the vLLM engine.</p> <p>Here is an example of an inference configuration using the vLLM engine:</p> <pre><code>model:\n    _target_: ragfit.models.vllm.VLLMInference\n    model_name_or_path: \"facebook/opt-125m\"\n    llm_params:\n        dtype: auto\n    generation:\n        temperature: 0.5\n        top_p: 0.95\n        seed: 1911\n    num_gpus: 1\n\ndata_file: my-processed-data.jsnol\ngenerated_file: model-predictions.jsonl\ninput_key: prompt\ngeneration_key: output\ntarget_key: answers\nlimit:\n</code></pre> <p>The main differences in this configuration are as follows:</p> <ul> <li><code>ragfit.models.vllm.VLLMInference</code>: This class is used to utilize the vLLM-based engine.</li> <li><code>llm_params</code>: These are optional vLLM arguments that can be passed to the LLM class.</li> <li><code>generation</code>: These are optional arguments that define the generation policy. The supported arguments are compatible with vLLM's <code>SamplingParams</code>.</li> <li><code>num_gpus</code>: This specifies the number of GPUs to use during inference.</li> </ul>"},{"location":"processing/","title":"Data Augmentation","text":"<p>To demonstrate the usage of RAG-FiT data augmentation, we will follow the experimentation presented in the paper. Choosing the ASQA Q&amp;A dataset and the Phi-3 model. We compare a baseline configuration with 4 other configurations:</p> <ol> <li>Retrieval augmentation using a corpus and inserting the documents in the prompt after the question.</li> <li>Similar to (1) but having the model fine-tune on the completions.</li> <li>Similar to (1) and adding a Chain-of-Thought instruction for the model to explain its reasoning and format its answer.</li> <li>Similar to (3) but having the model fine-tune on the completions while implementing a technique from RAFT where distracting documents are used.</li> </ol> <p>The ASQA dataset has two types of answer: a long answer and lists of short answers (actually list of lists). Additionally, it has some minimal amount of context in the data, so we augment it using a corpus, stored as a vector DB; we use Qdrant.</p> <p>In order to train configuration (4), we need to have CoT well-reasoned responses as labels, so we use OpenAI GPT4 model to augment a dataset with these synthetic labels.</p> <p>Notice: all the configurations mentioned here, implementing the experiments done in the paper, are saved in <code>configs/paper/</code>. They don't run by default, they need to be specified by running:</p> <pre><code>python module-name.py -cp configs/paper -cn config-name-without-extension\n</code></pre>"},{"location":"processing/#retrieval","title":"Retrieval","text":"<p>The first step would be to augment the entire dataset (train, dev) with relevant documents, based on the questions, see processing-asqa-retrieval.yaml. Let's focus on the different steps:</p> <pre><code>- _target_: ragfit.processing.dataset_loaders.loaders.HFLoader\n  inputs: train\n  dataset_config:\n        path: din0s/asqa\n        split: train\n\n- _target_: ragfit.processing.dataset_loaders.loaders.HFLoader\n  inputs: dev\n  dataset_config:\n        path: din0s/asqa\n        split: dev\n</code></pre> <p>We load the train and dev splits, to be used in the pipeline; they will be referred using the <code>inputs</code> keyword used in this step.</p> <p><pre><code>- _target_: ragfit.processing.local_steps.common_datasets.ASQA\n  inputs: [train, dev]\n</code></pre> We do some minimal processing, related to ASQA, namely column renaming, collecting the short and long answers and having a consistent scheme, for example: <code>query</code>, <code>answers</code>, <code>positive_passages</code>, etc. Feel free to add your own types of pre-processing.</p> <p>Notice the <code>inputs</code> keyword can accept a list of strings, meaning the step will run over the datasets specified.</p> <p><pre><code>- _target_:\n        ragfit.processing.local_steps.retrievers.haystack.HaystackRetriever\n  inputs: [train, dev]\n  pipeline_or_yaml_path: ./configs/external/haystack/qdrant.yaml\n  docs_key: positive_passages\n  query_key: query\n</code></pre> This is the retrieval step. We use the Haystack framework for building RAG pipelines; in this example, the Haystack pipeline is comprised of an embedder and a retriever, connecting the Qdrant using a Qdrant-Haystack integration (all defined in the requirements file). The Haystack pipeline is initialized from the Qdrant.yaml configuration. One can use other frameworks for retrieval, like LangChain, LlamaIndex, or others.</p> <p>The retrieval step will store the most relevant documents (k=5) in the <code>docs_key</code> and the query will be defined by the <code>query_key</code>.</p> <p><pre><code>- _target_: ragfit.processing.local_steps.context.ContextHandler\n  inputs: [train, dev]\n  docs_key: positive_passages\n</code></pre> In this simple step, the documents retrieved are processed; they have a title and content fields and this step combine these into a single string for every document. This step may be unnecessary, depending on the retrieval mechanism and format.</p> <p><pre><code>- _target_: ragfit.processing.global_steps.sampling.Sampler\n  inputs: [train, dev]\n  k: 1\n  input_key: positive_passages\n  output_key: negative_passages\n</code></pre> The <code>Sampler</code> class deals with sampling examples from the same dataset or others. In order to train the RAFT-based model on a combination of relevant and distracting documents, we need to collect these distracting documents. Here we chose to collect positive documents from other examples, to be used as negative documents. The <code>Sampler</code> is then ran with k=1, it collects only the <code>positive_passages</code> from the examples it samples and store them in a new keyword, called <code>negative_passages</code>.</p> <p><pre><code>- _target_: ragfit.processing.global_steps.output.OutputData\n  inputs: [train, dev]\n  prefix: asqa\n</code></pre> Finally we write the two resulting dataset to disk. They represent the retrieval-augmented datasets, ready to be processed for the different tasks.</p> <p>To run this process: <pre><code>python processing.py -cp configs/paper -cn processing-asqa-retrieval\n</code></pre></p>"},{"location":"processing/#baseline-configuration","title":"Baseline Configuration","text":"<p>For the baseline, there is not going to be context, only the question presented to the model. We use instruction-following models that have a chat template built-in. The framework populates the chat template using the inputs and outputs we generate, so we don't need to worry about roles and special tokens. Additionally, the system instruction is specified only during training and inference: it needn't be part of the dataset so these next steps mainly deal with the prompt generation.</p> <p>These are the interesting steps:</p> <pre><code>- _target_: ragfit.processing.dataset_loaders.loaders.LocalLoader\n  inputs: dev\n  filename: asqa-dev.jsonl\n\n- _target_: ragfit.processing.local_steps.prompter.TextPrompter\n  inputs: dev\n  prompt_file: ragfit/processing/prompts/qa-short.txt\n  output_key: prompt\n  mapping:\n        query: query\n</code></pre> <p>We load the locally retrieval-augmented files we generated in the previous section.</p> <p>The <code>TextPrompter</code> populates a template file containing placeholders in python format, see the short template. The step replace the placeholders with variables using a provided mapping. The result is a string, saved in a keyword called <code>outputs_key</code>.</p> <p>To run this process: <pre><code>python processing.py -cp configs/paper -cn processing-asqa-baseline\n</code></pre></p>"},{"location":"processing/#context","title":"Context","text":"<p>Preparing for configurations (1) and (2), we want to augment the examples with the top 5 documents we collected in the first step.</p> <p><pre><code>- _target_: ragfit.processing.local_steps.context.DocumentsJoiner\n  inputs: [train, dev]\n  docs_key: positive_passages\n  k: 5\n\n- _target_: ragfit.processing.local_steps.prompter.TextPrompter\n  inputs: [train, dev]\n  prompt_file: ragfit/processing/prompts/qa.txt\n  output_key: prompt\n  mapping:\n        question: query\n        context: positive_passages\n</code></pre> The <code>DocumentJoiner</code> joins a list of strings and is needed before the <code>TextPrompter</code> we've seen from the previous section. We prepare a dev file\u2014for testing the model with retrieved documents\u2014and also a training file, in order to run fine-tuning. Both configurations will be evaluated on the dev dataset.</p> <p>To run this process: <pre><code>python processing.py -cp configs/paper -cn processing-asqa-context\n</code></pre></p>"},{"location":"processing/#chain-of-thought","title":"Chain-of-Thought","text":"<p>We prepare a dev set with CoT reasoning prompt. The configuration will be similar to the Context configuration, however here we use a different prompt template:</p> <pre><code>- _target_: ragfit.processing.local_steps.prompter.TextPrompter\n  inputs: dev\n  prompt_file: ragfit/processing/prompts/cot.txt\n  output_key: prompt\n  mapping:\n        question: query\n        context: positive_passages\n</code></pre> <p>To run this process: <pre><code>python processing.py -cp configs/paper -cn processing-asqa-cot-dev\n</code></pre></p>"},{"location":"processing/#chain-of-thought-training-dataset","title":"Chain-of-Thought Training Dataset","text":"<p>In order to train a model on a CoT-based prompt, we need to collect well-reasoned responses; we use GPT4 for that. Additionally, we implement a technique from RAFT where some percentage of the examples have purely distractor documents, in order for the model ability to filter noise. Here are the relevant steps:</p> <p><pre><code>- _target_: ragfit.processing.local_steps.raft.RAFTStep\n  inputs: train\n  k: 5\n  raft_p: 0.5\n  neg_docs_num: 2\n  output_key: raft_docs\n</code></pre> The <code>RAFTStep</code> implements the logic presented in the paper; the percentage of purely-distractor documents is defined by <code>raft_p</code>. The list of documents, some relevant, some distracting, are saved in a keyword called <code>output_key</code>.</p> <p><pre><code>- _target_: ragfit.processing.local_steps.context.DocumentsJoiner\n  inputs: train\n  docs_key: raft_docs\n  k:\n\n- _target_: ragfit.processing.local_steps.prompter.TextPrompter\n  inputs: train\n  prompt_file: ragfit/processing/prompts/cot.txt\n  output_key: prompt\n  mapping:\n        question: query\n        context: raft_docs\n</code></pre> The documents are joined into strings; when <code>k:</code> all documents are used. The prompt used is the same as when building the dev dataset.</p> <p>Next is interacting with OpeanAI; we implemented an OpenAI class using Azure, one can implement using other abstractions. The step itself needs the <code>prompt_key</code>, instruction file and the results are saved in the <code>answer_key</code>. <pre><code>- _target_: ragfit.processing.local_steps.api.openai.OpenAIChat\n  inputs: train\n  prompt_key: prompt\n  answer_key: generated_answer\n  instruction: ragfit/processing/prompts/prompt_instructions/qa.txt\n  model:\n        azure_endpoint: azure.endpoint.com\n        api_version: 2024-05-01-preview\n        model: GPT-4-32k-Bot\n</code></pre></p> <p>To run this process: <pre><code>python processing.py -cp configs/paper -cn processing-asqa-cot-train\n</code></pre></p>"},{"location":"pubmed/","title":"Fine-tuning Phi-3 for PubmedQA","text":"<p>We will demonstrate the RAG-FiT framework by creating a RAG augmented dataset, fine-tuning a model and running an evaluation on the PubmedQA dataset. We will follow the experimentation in the paper, implementing the RAG-sft configuration, which comprised of creating prompts with relevant context and fine-tuning a model on the completions.</p> <p>The PubmedQA dataset contains relevant context for each question, so there's no need for retrieval\u2014for an example with a retrieval step, see the ASQA processing tutorial.</p> <p>Notice: all the configurations mentioned in this guide, implementing the experiments done in the paper, are saved in <code>configs/paper/</code>. They don't run by default, they need to be specified by running:</p> <pre><code>python module-name.py -cp configs/paper -cn config-name-without-extension\n</code></pre>"},{"location":"pubmed/#rag-dataset-creation","title":"RAG Dataset Creation","text":"<p>We use the 1<sup>st</sup> module, called <code>processing.py</code> to generate the RAG-augmented dataset. To run it:</p> <pre><code>python processing.py -cp configs/paper -cn processing-pubmed-context\n</code></pre> <p>Let's analyze the configuration file used for the dataset creation:</p> <pre><code>name: pubmed_rag\ncache: true\noutput_path: .\n</code></pre> <p>Start by defining a pipeline name, turning caching on, and specifying the current folder for the output files.</p> <pre><code>steps:\n    - _target_: ragfit.processing.dataset_loaders.loaders.HFLoader\n      inputs: train\n      dataset_config:\n            path: bigbio/pubmed_qa\n            split: train\n\n    - _target_: ragfit.processing.dataset_loaders.loaders.HFLoader\n      inputs: test\n      dataset_config:\n            path: bigbio/pubmed_qa\n            name: pubmed_qa_labeled_fold0_source\n            split: test\n</code></pre> <p>Next we load a training and test sets from the Hugging Face hub. The <code>inputs</code> keyword is used to denote the datasets to be used on the subsequent steps.</p> <pre><code>    - _target_: ragfit.processing.global_steps.sampling.ShuffleSelect\n      inputs: train\n      limit: 50000\n\n    - _target_: ragfit.processing.local_steps.common_datasets.PubMed\n      inputs: [train, test]\n\n    - _target_: ragfit.processing.local_steps.context.DocumentsJoiner\n      inputs: [train, test]\n      docs_key: positive_passages\n      k: 5\n</code></pre> <p>Next are 3 technical steps: we limit the size of the training dataset to 50k examples (optional). We do minimal processing of features: namely creating a <code>query</code>, <code>answers</code> and <code>positive_passages</code> features. Finally, we combine <code>k=5</code> relevant documents for each example into a string, to be used later in a prompt.</p> <pre><code>    - _target_: ragfit.processing.local_steps.prompter.TextPrompter\n      inputs: [train, test]\n      prompt_file: ragfit/processing/prompts/qa.txt\n      output_key: prompt\n      mapping:\n            question: query\n            context: positive_passages\n</code></pre> <p>Next is the prompt generation step; we used a QA prompt with <code>question</code> and <code>context</code> placeholders. We map the values using the <code>mapping</code> keyword.</p> <p>[!IMPORTANT] There is no model-dependency in the prompt building. For models/tokenizers supporting a chat format, the prompt is going to be uttered by the user role, where the chat, including a system instruction, is constructed only in the training and inference stages. For models/tokenizers not supporting a chat format, a template can be provided by the users, to be used in the training and inference stages.</p> <p>Finally we write the results to files.</p>"},{"location":"pubmed/#training","title":"Training","text":"<p>Training is done on the generated files. The training configuration has 3 parts: model, training arguments and data.</p> <pre><code>model:\n    _target_: ragfit.models.hf.HFTrain\n    model_name_or_path: microsoft/Phi-3-mini-128k-instruct\n    load_in_4bit: false\n    load_in_8bit: true\n    lora:\n        lora_alpha: 16\n        lora_dropout: 0.1\n        peft_type: LORA\n        r: 16\n        target_modules:\n            - qkv_proj\n        task_type: CAUSAL_LM\n    completion_start: &lt;|assistant|&gt;\n    instruction_in_prompt:\n    max_sequence_len: 2000\n</code></pre> <p>Model loading is implemented using the <code>HFTrain</code> class, which loads models from HuggingFace hub and uses PEFT adapters. Other classes can be implemented. The important keys here are: <code>completion_start</code> which indicates the beginning of the text where loss is to be calculated. This is model/tokenizer specific. Additionally, there is the <code>instruction_in_prompt</code> key, which if set to True, inserts the system instruction in the prompt, for models which do not support a dedicated system role.</p> <pre><code>train:\n    output_dir: ./trained_model/\n    gradient_accumulation_steps: 2\n    learning_rate: 1e-4\n    logging_steps: 10\n    lr_scheduler_type: cosine\n    num_train_epochs: 1\n    per_device_train_batch_size: 1\n    optim: paged_adamw_8bit\n    warmup_ratio: 0.03\n    weight_decay: 0.001\n</code></pre> <p>Training is done using the <code>SFTTrainer</code> in <code>TRL</code>. Training arguments are based on HuggingFace <code>Trainer</code>.</p> <pre><code>instruction: ragfit/processing/prompts/prompt_instructions/qa-yes-no.txt\ntemplate:\ndata_file: pubmed-rag-train.jsonl\ninput_key: prompt\noutput_key: answers\nlimit:\nshuffle:\nhfhub_tag:\n</code></pre> <p>Here are they important keys:</p> <ul> <li>The instruction file to use for training (should later be used for inference as well). In the case of PubmedQA, the answers are either Yes or No, so we specify this in the system instruction.</li> <li>If the model/tokenizer do not support a chat template, the user needs to provided a custom template; they placeholders to fill are <code>query</code> and <code>output</code>.</li> <li>Data file is the processed file to train on.</li> <li>Input key is the prompt.</li> <li>Output key is completion text to learn.</li> <li>Limit and shuffle can be used to filter the dataset for debugging purposes.</li> <li>The framework can push the trained model to <code>hfhub_tab</code>.</li> </ul> <p>We create a training job by running:</p> <pre><code>python training.py -cp configs/paper -cn training-pubmed\n</code></pre>"},{"location":"pubmed/#inference","title":"Inference","text":"<p>In the inference stage, we take the processed dataset and LLM and make predictions. The LLM can be fine-tuned. The processed data encapsulates the RAG interactions: pre-processing, retrieval, ranking, prompt-creation, and possibly other types of transformations. So this step deals with producing the predictions to be evaluated.</p> <p>It is simple in nature, described by the following configuration:</p> <pre><code>model:\n    _target_: ragfit.models.hf.HFInference\n    model_name_or_path: microsoft/Phi-3-mini-128k-instruct\n    load_in_4bit: false\n    load_in_8bit: true\n    device_map: auto\n    trust_remote_code: true\n    instruction: ragfit/processing/prompts/prompt_instructions/qa-yes-no.txt\n    lora_path: ./trained_model/checkpoint\n    generation:\n        do_sample: false\n        max_new_tokens: 50\n        return_full_text: false\n\ndata_file: pubmed-rag-test.jsonl\ngenerated_file: pubmed-rag-test-generated.jsonl\ninput_key: prompt\ngeneration_key: output\ntarget_key: answers\nlimit:\n</code></pre> <p>The model section deals with details regarding the model loading and generation options. System instruction can be provided, as we mentioned previously: the datasets are model independent, and all model details (system instruction, custom chat template) are needed only during training and inference. Similarly, <code>instruct_in_prompt</code> inserts the system instruction inside the prompt, for models which don't support a system role.</p> <p>Other parameters:</p> <ul> <li>Data file is the processed file.</li> <li>Generated file is the file that will be created with the completions (and labels, for easy debugging).</li> <li>Target key is the label keyword.</li> <li>Limit: to a number of examples, for debugging.</li> </ul> <p>In order to run inference:</p> <pre><code>python inference.py -cp configs/paper -cn inference-pubmed\n</code></pre>"},{"location":"pubmed/#evaluations","title":"Evaluations","text":"<p>The evaluation module takes the produced inference file and the original processed dataset and runs a list of evaluations, producing a final results file, in a YAML format. The evaluations are represented as metric classes.</p> <p>We implement several metrics including: a wrapper for HuggingFace <code>evaluate</code> class, which can accept a list of metrics, EM, F1, classification (accuracy, precision, recall, F1), BERTScore, Semantic similarity (using a customizable cross-encoder). The module can also run metrics from DeepEval, which offers a large collection of LLM evaluations.</p> <p>The configuration for the evaluation looks like this:</p> <pre><code>answer_processor:\n    _target_: ragfit.processing.answer_processors.regex.RegexAnswer\n    capture_pattern:\n    stopping_pattern:\n\nmetrics:\n    - _target_: ragfit.evaluation.metrics.Classification\n      mapping:\n        \"yes\": 1\n        \"no\": 0\n        \"maybe\": 2\n      else_value: 2\n\nkey_names:\n    generated: text\n    label: answers\n    query: query\n\nresults_file: evaluation-pubmed-rag.yaml\ngenerated_file: pubmed-rag-test-generated.jsonl\ndata_file: pubmed-rag-test.jsonl\nlimit:\n</code></pre> <p>The evaluation module introduces the concept of an Answer Processor. This class can run post-processing on the generated text, including: aligning text with the expect output, implement evaluation-specific formatting, extracting the specific sections, processing meta-data like citations, etc.</p> <p>The default processor is called <code>RegexAnswer</code>; it can filter text, based on a python regex capture pattern. It can also split text using a stopping pattern. For example, in the Chain-of-Thought reasoning we used in the paper, the model is instruction to explain its answer, cite if needed and finally print the final results in the following format <code>&lt;ANSWER&gt;: ...</code>. We can use this format as a capture pattern; thus models that learn to answer using this pattern (obey the instruction) will score higher.</p> <p>For PubmedQA we use a classification metric; we provide a mapping of keys and a default key, since the PubmedQA expert annotated test set can contain Yes, No or Maybe, as answers.</p> <p>The rest of the arguments are straightforward:</p> <ul> <li>Keyword names for input, output and target.</li> <li>Name of inference file, name of the processed data.</li> <li>Name for the results summary report.</li> <li>Limit, for debugging purposes.</li> </ul> <p>Running the evaluation:</p> <pre><code>python evaluation.py -cp configs/paper -cn evaluation-pubmed\n</code></pre>"},{"location":"pubmed/#summary","title":"Summary","text":"<p>In this tutorial, we enhanced an LLM to better perform Q&amp;A on the PubmedQA task, by generating a training dataset containing relevant context, fine-tuning and evaluating the model on the testset. By modifying the configurations presented here, one can run an evaluation on an untrained model and see the benefit of RAG. One can implement other RAG techniques; for example, see the ASQA tutorial for a more advanced usecase (as well as more thorough explanations), including external retrieval, OpenAI integration and Chain-of-thought prompting: data creation, training, inference and evaluation.</p>"},{"location":"training/","title":"Training","text":"<p>Training is done on the processed files. The training configuration has 3 parts: model, training arguments and data.</p> <p><pre><code>model:\n    _target_: ragfit.models.hf.HFTrain\n    model_name_or_path: microsoft/Phi-3-mini-128k-instruct\n    load_in_4bit: false\n    load_in_8bit: true\n    lora:\n        bias: none\n        fan_in_fan_out: false\n        lora_alpha: 16\n        lora_dropout: 0.1\n        peft_type: LORA\n        r: 16\n        target_modules:\n            - qkv_proj\n        task_type: CAUSAL_LM\n        use_rslora: true\n    completion_start: &lt;|assistant|&gt;\n    instruction_in_prompt:\n    max_sequence_len: 4000\n</code></pre> Model loading is done in the <code>HFTrain</code> class, which loads models from HuggingFace hub and uses PEFT adapters. Other classes can be implemented. The important keys here are: <code>completion_start</code> which indicates the beginning of the text where loss is to be calculated. This is model/tokenizer specific. Additionally, there is the <code>instruction_in_prompt</code> key, which if set to True, inserts the system instruction in the prompt, for models which do not support a dedicated system role.</p> <p>Next is the training arguments: <pre><code>train:\n    output_dir: ./trained_models/\n    bf16: false\n    fp16: false\n    gradient_accumulation_steps: 2\n    group_by_length:\n    learning_rate: 1e-4\n    logging_steps: 10\n    lr_scheduler_type: cosine\n    max_steps: -1\n    num_train_epochs: 1\n    per_device_train_batch_size: 1\n    optim: paged_adamw_8bit\n    remove_unused_columns: true\n    save_steps: 20000\n    save_total_limit: 1\n    warmup_ratio: 0.03\n    weight_decay: 0.001\n    report_to:\n</code></pre></p> <p>Training is done using the <code>SFTTrainer</code> in <code>TRL</code>. Training arguments are based on HuggingFace <code>Trainer</code>.</p> <p>Finally, data and other options: <pre><code>instruction: ragfit/processing/prompts/prompt_instructions/qa.txt\ntemplate:\ndata_file:\ninput_key: prompt\noutput_key:\nresume_checkpoint:\nlimit:\nshuffle:\nhfhub_tag:\nuse_wandb:\nexperiment:\nwandb_entity:\n</code></pre></p> <p>Here are they important keys:</p> <ul> <li>The instruction file to use for training (should later be used for inference as well).</li> <li>If the model/tokenizer do not support a chat template, the user needs to provided a custom template; they placeholders to fill are <code>query</code> and <code>output</code>.</li> <li>Data file is the processed file to train on.</li> <li>Input key is the prompt.</li> <li>Output key is completion text to learn.</li> <li>Limit and shuffle can be used to filter the dataset for debugging purposes.</li> <li>The framework can push the trained model to <code>hfhub_tab</code>.</li> <li>The last three keys related to experiment tracking using WANDB. Other services can be used by modifying the <code>report_to</code> key.</li> </ul>"},{"location":"training/#sending-runs","title":"Sending Runs","text":"<p>As we mentioned in the Data Augmentation page, we demonstrate the framework functionality using the ASQA dataset and the Phi-3 model, experimenting with 5 different configurations. Only 2 configurations require fine-tuning. One can send the training job like this:</p> <pre><code>python training.py -cp configs/paper -cn training-asqa   \\\n       data_file=asqa-context-train.jsonl                \\\n       output_key=answers                                \\\n       train.output_dir=./trained_models_context/\n</code></pre> <p>The <code>-cp</code> and <code>-cn</code> are overrides for the default configuration, which is <code>./configs/training.yaml</code>. Then there are overrides for the processed data file to use, the name of the label key and where to save the trained model. Overrides are based on the Hydra vocabulary.</p> <p>For the CoT model with RAFT contexts, we run: <pre><code>python training.py -cp configs/paper -cn training-asqa  \\\n       data_file=asqa-raft-cot-train.jsonl              \\\n       output_key=generated_answer                      \\\n       train.output_dir=./trained_models_cot/\n</code></pre></p>"},{"location":"reference/utils/","title":"Utils","text":""},{"location":"reference/utils/#ragfit.utils.check_package_installed","title":"<code>check_package_installed(package_name: str, optional_msg: str = '')</code>","text":"<p>Check if a package is installed.</p> Source code in <code>ragfit/utils.py</code> <pre><code>def check_package_installed(package_name: str, optional_msg: str = \"\"):\n    \"\"\"\n    Check if a package is installed.\n    \"\"\"\n\n    import importlib.util\n\n    if importlib.util.find_spec(package_name) is None:\n        raise ImportError(f\"{package_name} package is not installed; {optional_msg}\")\n</code></pre>"},{"location":"reference/evaluation/base/","title":"Base","text":""},{"location":"reference/evaluation/base/#ragfit.evaluation.base.MetricBase","title":"<code>MetricBase</code>","text":"<p>Base class for metrics.</p> <p>Metrics can be local or global; local means score are calculated per example. Global means score is calculated by looking at the entire dataset, e.g. fluency.</p> Source code in <code>ragfit/evaluation/base.py</code> <pre><code>class MetricBase:\n    \"\"\"\n    Base class for metrics.\n\n    Metrics can be local or global; local means score are calculated per example.\n    Global means score is calculated by looking at the entire dataset, e.g. fluency.\n    \"\"\"\n\n    def __init__(self, key_names, **kwargs):\n        self.key_names = key_names\n        self.kwargs = kwargs\n        self.field = self.key_names[\"generated\"]\n        self.target = self.key_names[\"label\"]\n\n    def measure(self, example: dict) -&gt; dict[str, float]:\n        \"\"\"\n        Measure the performance of the model on a given example.\n\n        Parameters:\n            example (dict): The example to evaluate the model on.\n\n        Returns:\n            dict[str, float]: A dictionary containing the performance metrics.\n\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/evaluation/base/#ragfit.evaluation.base.MetricBase.measure","title":"<code>measure(example: dict) -&gt; dict[str, float]</code>","text":"<p>Measure the performance of the model on a given example.</p> <p>Parameters:</p> <ul> <li> <code>example</code>               (<code>dict</code>)           \u2013            <p>The example to evaluate the model on.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, float]</code>           \u2013            <p>dict[str, float]: A dictionary containing the performance metrics.</p> </li> </ul> Source code in <code>ragfit/evaluation/base.py</code> <pre><code>def measure(self, example: dict) -&gt; dict[str, float]:\n    \"\"\"\n    Measure the performance of the model on a given example.\n\n    Parameters:\n        example (dict): The example to evaluate the model on.\n\n    Returns:\n        dict[str, float]: A dictionary containing the performance metrics.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/evaluation/deep/","title":"DeepEval","text":""},{"location":"reference/evaluation/deep/#ragfit.evaluation.deep.DeepEvalBase","title":"<code>DeepEvalBase</code>","text":"<p>               Bases: <code>MetricBase</code></p> <p>Base class for DeepEval metrics.</p> <p>Here we use AzureChatOpenAI interface; replace if needed.</p> Source code in <code>ragfit/evaluation/deep.py</code> <pre><code>class DeepEvalBase(MetricBase):\n    \"\"\"\n    Base class for DeepEval metrics.\n\n    Here we use AzureChatOpenAI interface; replace if needed.\n    \"\"\"\n\n    def __init__(\n        self,\n        key_names: dict,\n        api_version,\n        azure_endpoint,\n        azure_deployment,\n        **kwargs,\n    ):\n        super().__init__(key_names, **kwargs)\n        from deepeval.test_case import LLMTestCase\n        from langchain_openai import AzureChatOpenAI\n\n        self.local = True\n        self.query = self.key_names[\"query\"]\n        self.context = self.key_names[\"context\"]\n        self.test_case = LLMTestCase\n\n        self.model = AzureChatOpenAI(\n            api_version=api_version,\n            azure_endpoint=azure_endpoint,\n            azure_deployment=azure_deployment,\n            request_timeout=600,\n            max_retries=10,\n        )\n</code></pre>"},{"location":"reference/evaluation/deep/#ragfit.evaluation.deep.Faithfulness","title":"<code>Faithfulness</code>","text":"<p>               Bases: <code>DeepEvalBase</code></p> <p>Faithfulness metric from DeepEval, based on RAGAS.</p> <p>Measures faithfulness of generated text by comparing it to the target text.</p> Source code in <code>ragfit/evaluation/deep.py</code> <pre><code>class Faithfulness(DeepEvalBase):\n    \"\"\"\n    Faithfulness metric from DeepEval, based on RAGAS.\n\n    Measures faithfulness of generated text by comparing it to the target text.\n    \"\"\"\n\n    def __init__(self, key_names: dict, threshold=0.3, **kwargs):\n        super().__init__(key_names, **kwargs)\n        from deepeval.metrics.ragas import RAGASFaithfulnessMetric\n\n        self.metric = RAGASFaithfulnessMetric(threshold=threshold, model=self.model)\n\n    def measure(self, example):\n        query = example[self.query]\n        output = example[self.field]\n        context = example[self.context]\n\n        test_case = self.test_case(\n            input=query,\n            actual_output=output or \"No answer.\",\n            retrieval_context=[context] if isinstance(context, str) else context,\n        )\n        try:\n            self.metric.measure(test_case)\n            score = self.metric.score\n        except Exception as e:\n            logging.error(f\"OpenAI exception: {e}\")\n            score = 0\n\n        return {\"faithfulness\": score if not math.isnan(score) else 0}\n</code></pre>"},{"location":"reference/evaluation/deep/#ragfit.evaluation.deep.Hallucination","title":"<code>Hallucination</code>","text":"<p>               Bases: <code>DeepEvalBase</code></p> <p>Hallucination metric from DeepEval.</p> <p>Measures hallucination of generated text by comparing it to the retrieved documents.</p> Source code in <code>ragfit/evaluation/deep.py</code> <pre><code>class Hallucination(DeepEvalBase):\n    \"\"\"\n    Hallucination metric from DeepEval.\n\n    Measures hallucination of generated text by comparing it to the retrieved documents.\n    \"\"\"\n\n    def __init__(self, key_names: dict, threshold=0.5, **kwargs):\n        super().__init__(key_names, **kwargs)\n        from deepeval.metrics import HallucinationMetric\n\n        self.metric = HallucinationMetric(\n            threshold=threshold, include_reason=False, model=self.model\n        )\n\n    def measure(self, example):\n        output = example[self.field]\n        context = example[self.context]\n\n        test_case = self.test_case(\n            input=\"\",\n            actual_output=output,\n            context=[context] if isinstance(context, str) else context,\n        )\n\n        try:\n            self.metric.measure(test_case)\n            score = self.metric.score\n        except Exception as e:\n            logging.error(f\"OpenAI exception: {e}\")\n            score = 0\n\n        return {\"hallucination\": score}\n</code></pre>"},{"location":"reference/evaluation/deep/#ragfit.evaluation.deep.Relevancy","title":"<code>Relevancy</code>","text":"<p>               Bases: <code>DeepEvalBase</code></p> <p>Answer relevancy metric from DeepEval, based on RAGAS.</p> <p>Measures relevancy of generated text by comparing it to the retrieved documents.</p> Source code in <code>ragfit/evaluation/deep.py</code> <pre><code>class Relevancy(DeepEvalBase):\n    \"\"\"\n    Answer relevancy metric from DeepEval, based on RAGAS.\n\n    Measures relevancy of generated text by comparing it to the retrieved documents.\n    \"\"\"\n\n    def __init__(self, key_names: dict, embeddings, threshold=0.3, **kwargs):\n        super().__init__(key_names, **kwargs)\n        from deepeval.metrics.ragas import RAGASAnswerRelevancyMetric\n        from ragas.embeddings import HuggingfaceEmbeddings\n\n        self.metric = RAGASAnswerRelevancyMetric(\n            threshold=threshold,\n            embeddings=HuggingfaceEmbeddings(model_name=embeddings),\n            model=self.model,\n        )\n\n    def measure(self, example):\n        query = example[self.query]\n        output = example[self.field]\n        context = example[self.context]\n\n        test_case = self.test_case(\n            input=query,\n            actual_output=output or \"No answer.\",\n            retrieval_context=[context] if isinstance(context, str) else context,\n        )\n        try:\n            self.metric.measure(test_case)\n            score = self.metric.score\n        except Exception as e:\n            logging.error(f\"OpenAI exception: {e}\")\n            score = 0\n\n        return {\"relevancy\": score}\n</code></pre>"},{"location":"reference/evaluation/metrics/","title":"Metrics","text":""},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.BERTScore","title":"<code>BERTScore</code>","text":"<p>               Bases: <code>MetricBase</code></p> <p>BERTScore metric, based on the BERTScore library.</p> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>class BERTScore(MetricBase):\n    \"\"\"\n    BERTScore metric, based on the BERTScore library.\n    \"\"\"\n\n    def __init__(self, key_names: dict, model=\"microsoft/deberta-large-mnli\", **kwargs):\n        \"\"\"Initialize the Metrics class.\n\n        Args:\n            key_names (dict): A dictionary containing the field names.\n            model (str, optional): The name of the BERT model to use. Defaults to \"microsoft/deberta-large-mnli\".\n        \"\"\"\n        super().__init__(key_names, **kwargs)\n        from bert_score import BERTScorer\n\n        self.scorer = BERTScorer(model, lang=\"en\", rescale_with_baseline=True)\n        self.local = True\n\n    def measure(self, example):\n        input = example[self.field]\n        target = example[self.target]\n\n        if not isinstance(target, list):\n            target = [target]\n\n        scores = [self.scorer.score([input], [t])[2].item() for t in target]\n\n        return {\"BERTScore-F1\": max(scores)}\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.BERTScore.__init__","title":"<code>__init__(key_names: dict, model='microsoft/deberta-large-mnli', **kwargs)</code>","text":"<p>Initialize the Metrics class.</p> <p>Parameters:</p> <ul> <li> <code>key_names</code>               (<code>dict</code>)           \u2013            <p>A dictionary containing the field names.</p> </li> <li> <code>model</code>               (<code>str</code>, default:                   <code>'microsoft/deberta-large-mnli'</code> )           \u2013            <p>The name of the BERT model to use. Defaults to \"microsoft/deberta-large-mnli\".</p> </li> </ul> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>def __init__(self, key_names: dict, model=\"microsoft/deberta-large-mnli\", **kwargs):\n    \"\"\"Initialize the Metrics class.\n\n    Args:\n        key_names (dict): A dictionary containing the field names.\n        model (str, optional): The name of the BERT model to use. Defaults to \"microsoft/deberta-large-mnli\".\n    \"\"\"\n    super().__init__(key_names, **kwargs)\n    from bert_score import BERTScorer\n\n    self.scorer = BERTScorer(model, lang=\"en\", rescale_with_baseline=True)\n    self.local = True\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.Classification","title":"<code>Classification</code>","text":"<p>               Bases: <code>MetricBase</code></p> <p>Metrics for classification answers: accuracy, precision, recall, F1; macro-averaged.</p> dict - mapping of labels to integers. <p>Example: {\"true\": 1, \"false\": 0, \"maybe\": 2}</p> <p>else_value: int - value to assign to labels not in the mapping.</p> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>class Classification(MetricBase):\n    \"\"\"\n    Metrics for classification answers: accuracy, precision, recall, F1; macro-averaged.\n\n    mapping: dict - mapping of labels to integers.\n        Example: {\"true\": 1, \"false\": 0, \"maybe\": 2}\n    else_value: int - value to assign to labels not in the mapping.\n    \"\"\"\n\n    def __init__(\n        self, key_names: dict, mapping: dict, else_value: int = 2, **kwargs\n    ) -&gt; None:\n        from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n        super().__init__(key_names, **kwargs)\n        self.local = False\n        self.mapping = mapping\n        self.else_value = else_value\n        self.precision_recall_fn = precision_recall_fscore_support\n        self.accuracy_fn = accuracy_score\n\n    def in_text(self, text):\n        if \"yes\" in text:\n            return 1\n        if \"no\" in text:\n            return 0\n        return 2\n\n    def measure(self, example: dict):\n        inputs = example[self.field]\n        targets = example[self.target]\n\n        if isinstance(targets[0], list):\n            targets = [t[0] for t in targets]\n\n        inputs = [self.in_text(normalize_text(i).strip()) for i in inputs]\n\n        targets = [\n            self.mapping.get(normalize_text(t).strip(), self.else_value) for t in targets\n        ]\n\n        precision, recall, f1, _ = self.precision_recall_fn(\n            targets, inputs, average=\"macro\"\n        )\n        accuracy = self.accuracy_fn(targets, inputs)\n\n        return {\n            \"accuracy\": float(accuracy),\n            \"precision\": float(precision),\n            \"recall\": float(recall),\n            \"f1\": float(f1),\n        }\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.EM","title":"<code>EM</code>","text":"<p>               Bases: <code>MetricBase</code></p> <p>Implementing Exact Match based on code from Kilt.</p> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>class EM(MetricBase):\n    \"\"\"\n    Implementing Exact Match based on code from Kilt.\n    \"\"\"\n\n    def __init__(self, key_names, **kwargs) -&gt; None:\n        \"\"\"Initialize the Metrics class.\n\n        Args:\n            key_names (dict): A dictionary containing the field names.\n        \"\"\"\n        super().__init__(key_names, **kwargs)\n        self.local = True\n\n    def measure(self, example: dict):\n        input = example[self.field]\n        target = example[self.target]\n\n        assert isinstance(input, str), f\"Generated text should be a string: {input}\"\n        if not isinstance(target, list):\n            target = [target]\n\n        scores = [normalize_text(input) == normalize_text(t) for t in target]\n        return {\"EM\": int(max(scores))}\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.EM.__init__","title":"<code>__init__(key_names, **kwargs) -&gt; None</code>","text":"<p>Initialize the Metrics class.</p> <p>Parameters:</p> <ul> <li> <code>key_names</code>               (<code>dict</code>)           \u2013            <p>A dictionary containing the field names.</p> </li> </ul> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>def __init__(self, key_names, **kwargs) -&gt; None:\n    \"\"\"Initialize the Metrics class.\n\n    Args:\n        key_names (dict): A dictionary containing the field names.\n    \"\"\"\n    super().__init__(key_names, **kwargs)\n    self.local = True\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.F1","title":"<code>F1</code>","text":"<p>               Bases: <code>MetricBase</code></p> <p>Implementing F1 based on code from Kilt.</p> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>class F1(MetricBase):\n    \"\"\"\n    Implementing F1 based on code from Kilt.\n    \"\"\"\n\n    def __init__(self, key_names, **kwargs) -&gt; None:\n        \"\"\"Initialize the Metrics class.\n\n        Args:\n            key_names (dict): A dictionary containing the field names.\n        \"\"\"\n        super().__init__(key_names, **kwargs)\n        self.local = True\n\n    @staticmethod\n    def _f1(prediction, ground_truth):\n        prediction_tokens = normalize_text(prediction).split()\n        ground_truth_tokens = normalize_text(ground_truth).split()\n        common = Counter(prediction_tokens) &amp; Counter(ground_truth_tokens)\n        num_same = sum(common.values())\n        if num_same == 0:\n            return 0\n        precision = 1.0 * num_same / len(prediction_tokens)\n        recall = 1.0 * num_same / len(ground_truth_tokens)\n        f1 = (2 * precision * recall) / (precision + recall)\n        return f1\n\n    def measure(self, example: dict):\n        input = example[self.field]\n        target = example[self.target]\n\n        assert isinstance(input, str), f\"Generated text should be a string: {input}\"\n        if not isinstance(target, list):\n            target = [target]\n\n        scores = [self._f1(input, t) for t in target]\n        return {\"F1\": max(scores)}\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.F1.__init__","title":"<code>__init__(key_names, **kwargs) -&gt; None</code>","text":"<p>Initialize the Metrics class.</p> <p>Parameters:</p> <ul> <li> <code>key_names</code>               (<code>dict</code>)           \u2013            <p>A dictionary containing the field names.</p> </li> </ul> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>def __init__(self, key_names, **kwargs) -&gt; None:\n    \"\"\"Initialize the Metrics class.\n\n    Args:\n        key_names (dict): A dictionary containing the field names.\n    \"\"\"\n    super().__init__(key_names, **kwargs)\n    self.local = True\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.HFEvaluate","title":"<code>HFEvaluate</code>","text":"<p>               Bases: <code>MetricBase</code></p> <p>Wrapper class around <code>evaluate</code> metrics; easy to use, only need metric names.</p> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>class HFEvaluate(MetricBase):\n    \"\"\"\n    Wrapper class around `evaluate` metrics; easy to use, only need metric names.\n    \"\"\"\n\n    def __init__(self, key_names, metric_names: list[str], **kwargs):\n        \"\"\"\n        Args:\n            key_names (dict): A dictionary containing the field names.\n            metric_names (list[str]): A list of metric names.\n        \"\"\"\n        import evaluate\n\n        super().__init__(key_names, **kwargs)\n        self.metric_names = metric_names\n        self.metric = evaluate.combine(metric_names)\n        self.local = True\n\n    def measure(self, example):\n        \"\"\"\n        Measure the performance of the model on a given example.\n\n        Args:\n            example (dict): The example containing input and target values.\n\n        Returns:\n            dict: The performance metric(s) computed for the example.\n        \"\"\"\n        input = example[self.field]\n        target = example[self.target]\n\n        if isinstance(target, list):\n            results = defaultdict(int)\n            for tar in target:\n                results = {\n                    k: max(v, results[k])\n                    for k, v in self.metric.compute(\n                        predictions=[input], references=[tar]\n                    ).items()\n                }\n            return results\n        else:\n            return self.metric.compute(predictions=[input], references=[target])\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.HFEvaluate.__init__","title":"<code>__init__(key_names, metric_names: list[str], **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>key_names</code>               (<code>dict</code>)           \u2013            <p>A dictionary containing the field names.</p> </li> <li> <code>metric_names</code>               (<code>list[str]</code>)           \u2013            <p>A list of metric names.</p> </li> </ul> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>def __init__(self, key_names, metric_names: list[str], **kwargs):\n    \"\"\"\n    Args:\n        key_names (dict): A dictionary containing the field names.\n        metric_names (list[str]): A list of metric names.\n    \"\"\"\n    import evaluate\n\n    super().__init__(key_names, **kwargs)\n    self.metric_names = metric_names\n    self.metric = evaluate.combine(metric_names)\n    self.local = True\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.HFEvaluate.measure","title":"<code>measure(example)</code>","text":"<p>Measure the performance of the model on a given example.</p> <p>Parameters:</p> <ul> <li> <code>example</code>               (<code>dict</code>)           \u2013            <p>The example containing input and target values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>The performance metric(s) computed for the example.</p> </li> </ul> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>def measure(self, example):\n    \"\"\"\n    Measure the performance of the model on a given example.\n\n    Args:\n        example (dict): The example containing input and target values.\n\n    Returns:\n        dict: The performance metric(s) computed for the example.\n    \"\"\"\n    input = example[self.field]\n    target = example[self.target]\n\n    if isinstance(target, list):\n        results = defaultdict(int)\n        for tar in target:\n            results = {\n                k: max(v, results[k])\n                for k, v in self.metric.compute(\n                    predictions=[input], references=[tar]\n                ).items()\n            }\n        return results\n    else:\n        return self.metric.compute(predictions=[input], references=[target])\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.RecallEM","title":"<code>RecallEM</code>","text":"<p>               Bases: <code>MetricBase</code></p> <p>Implementing EM as in XRAG.</p> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>class RecallEM(MetricBase):\n    \"\"\"\n    Implementing EM as in XRAG.\n    \"\"\"\n\n    def __init__(self, key_names, **kwargs) -&gt; None:\n        \"\"\"Initialize the Metrics class.\n\n        Args:\n            key_names (dict): A dictionary containing the field names.\n        \"\"\"\n        super().__init__(key_names, **kwargs)\n        self.local = True\n\n    @staticmethod\n    def _normalize(text):\n        return unicodedata.normalize(\"NFD\", text)\n\n    def has_answer(self, answers, text, tokenizer=SimpleTokenizer()):\n        \"\"\"Check if a document contains an answer string.\"\"\"\n        text = self._normalize(text)\n        text = tokenizer.tokenize(text, uncased=True)\n\n        for answer in answers:\n            answer = self._normalize(answer)\n            answer = tokenizer.tokenize(answer, uncased=True)\n            for i in range(0, len(text) - len(answer) + 1):\n                if answer == text[i : i + len(answer)]:\n                    return True\n        return False\n\n    def measure(self, example: dict):\n        input = example[self.field]\n        target = example[self.target]\n\n        assert isinstance(input, str), f\"Generated text should be a string: {input}\"\n\n        if not isinstance(target, list):\n            target = [target]\n\n        scores = self.has_answer(target, input)\n        return {\"recallEM\": int(scores)}\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.RecallEM.__init__","title":"<code>__init__(key_names, **kwargs) -&gt; None</code>","text":"<p>Initialize the Metrics class.</p> <p>Parameters:</p> <ul> <li> <code>key_names</code>               (<code>dict</code>)           \u2013            <p>A dictionary containing the field names.</p> </li> </ul> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>def __init__(self, key_names, **kwargs) -&gt; None:\n    \"\"\"Initialize the Metrics class.\n\n    Args:\n        key_names (dict): A dictionary containing the field names.\n    \"\"\"\n    super().__init__(key_names, **kwargs)\n    self.local = True\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.RecallEM.has_answer","title":"<code>has_answer(answers, text, tokenizer=SimpleTokenizer())</code>","text":"<p>Check if a document contains an answer string.</p> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>def has_answer(self, answers, text, tokenizer=SimpleTokenizer()):\n    \"\"\"Check if a document contains an answer string.\"\"\"\n    text = self._normalize(text)\n    text = tokenizer.tokenize(text, uncased=True)\n\n    for answer in answers:\n        answer = self._normalize(answer)\n        answer = tokenizer.tokenize(answer, uncased=True)\n        for i in range(0, len(text) - len(answer) + 1):\n            if answer == text[i : i + len(answer)]:\n                return True\n    return False\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.Semantic","title":"<code>Semantic</code>","text":"<p>               Bases: <code>MetricBase</code></p> <p>Semantic similarity between label and answer using a cross-encoder.</p> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>class Semantic(MetricBase):\n    \"\"\"\n    Semantic similarity between label and answer using a cross-encoder.\n    \"\"\"\n\n    def __init__(\n        self,\n        key_names: dict,\n        model: str = \"vectara/hallucination_evaluation_model\",\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Initializes an instance of the class.\n\n        Args:\n            key_names (dict): A dictionary containing the field names.\n            model (str, optional): The name of the BERT model to use.\n        \"\"\"\n        super().__init__(key_names, **kwargs)\n\n        from sentence_transformers import CrossEncoder\n\n        self.model = CrossEncoder(model)\n        self.local = True\n\n    def measure(self, example):\n        input = example[self.field]\n        target = example[self.target]\n        if not isinstance(target, list):\n            target = [target]\n\n        scores = self.model.predict([[input, t] for t in target])\n\n        return {\"Semantic\": max(scores)}\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.Semantic.__init__","title":"<code>__init__(key_names: dict, model: str = 'vectara/hallucination_evaluation_model', **kwargs) -&gt; None</code>","text":"<p>Initializes an instance of the class.</p> <p>Parameters:</p> <ul> <li> <code>key_names</code>               (<code>dict</code>)           \u2013            <p>A dictionary containing the field names.</p> </li> <li> <code>model</code>               (<code>str</code>, default:                   <code>'vectara/hallucination_evaluation_model'</code> )           \u2013            <p>The name of the BERT model to use.</p> </li> </ul> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>def __init__(\n    self,\n    key_names: dict,\n    model: str = \"vectara/hallucination_evaluation_model\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Initializes an instance of the class.\n\n    Args:\n        key_names (dict): A dictionary containing the field names.\n        model (str, optional): The name of the BERT model to use.\n    \"\"\"\n    super().__init__(key_names, **kwargs)\n\n    from sentence_transformers import CrossEncoder\n\n    self.model = CrossEncoder(model)\n    self.local = True\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.SimpleTokenizer","title":"<code>SimpleTokenizer</code>","text":"<p>               Bases: <code>object</code></p> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>class SimpleTokenizer(object):\n    ALPHA_NUM = r\"[\\p{L}\\p{N}\\p{M}]+\"\n    NON_WS = r\"[^\\p{Z}\\p{C}]\"\n\n    def __init__(self):\n        \"\"\"\n        Args:\n            annotators: None or empty set (only tokenizes).\n        \"\"\"\n        self._regexp = regex.compile(\n            \"(%s)|(%s)\" % (self.ALPHA_NUM, self.NON_WS),\n            flags=regex.IGNORECASE + regex.UNICODE + regex.MULTILINE,\n        )\n\n    def tokenize(self, text, uncased=False):\n        matches = [m for m in self._regexp.finditer(text)]\n        if uncased:\n            tokens = [m.group().lower() for m in matches]\n        else:\n            tokens = [m.group() for m in matches]\n        return tokens\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.SimpleTokenizer.__init__","title":"<code>__init__()</code>","text":"<p>Parameters:</p> <ul> <li> <code>annotators</code>           \u2013            <p>None or empty set (only tokenizes).</p> </li> </ul> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Args:\n        annotators: None or empty set (only tokenizes).\n    \"\"\"\n    self._regexp = regex.compile(\n        \"(%s)|(%s)\" % (self.ALPHA_NUM, self.NON_WS),\n        flags=regex.IGNORECASE + regex.UNICODE + regex.MULTILINE,\n    )\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.StringEM","title":"<code>StringEM</code>","text":"<p>               Bases: <code>MetricBase</code></p> <p>Implementing String Exact Match.</p> <p>Used in ASQA to evaluate whether the annoated short answers appear in the generated answer as sub-strings.</p> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>class StringEM(MetricBase):\n    \"\"\"\n    Implementing String Exact Match.\n\n    Used in ASQA to evaluate whether the annoated short answers appear in the\n    generated answer as sub-strings.\n    \"\"\"\n\n    def __init__(self, key_names: dict, **kwargs) -&gt; None:\n        \"\"\"\n        Initialize the Metrics class.\n\n        Args:\n            key_names (dict): A dictionary containing the field names.\n        \"\"\"\n        super().__init__(key_names, **kwargs)\n        self.local = True\n\n    def measure(self, example: dict):\n        input = example[self.field]\n        target = example[self.target]\n\n        assert isinstance(input, str), f\"Generated text should be a string: {input}\"\n        assert isinstance(target[0], list), f\"Target should be a list of lists: {target}\"\n\n        input = normalize_text(input)\n        scores = [any(cand in input for cand in item) for item in target]\n\n        return {\"StringEM\": sum(scores) / len(scores)}\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.StringEM.__init__","title":"<code>__init__(key_names: dict, **kwargs) -&gt; None</code>","text":"<p>Initialize the Metrics class.</p> <p>Parameters:</p> <ul> <li> <code>key_names</code>               (<code>dict</code>)           \u2013            <p>A dictionary containing the field names.</p> </li> </ul> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>def __init__(self, key_names: dict, **kwargs) -&gt; None:\n    \"\"\"\n    Initialize the Metrics class.\n\n    Args:\n        key_names (dict): A dictionary containing the field names.\n    \"\"\"\n    super().__init__(key_names, **kwargs)\n    self.local = True\n</code></pre>"},{"location":"reference/evaluation/metrics/#ragfit.evaluation.metrics.normalize_text","title":"<code>normalize_text(s)</code>","text":"<p>Normalize the given text by lowercasing it, removing punctuation, articles, and extra whitespace.</p> <p>Parameters:</p> <ul> <li> <code>s</code>               (<code>str</code>)           \u2013            <p>The text to be normalized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>          \u2013            <p>The normalized text.</p> </li> </ul> Source code in <code>ragfit/evaluation/metrics.py</code> <pre><code>def normalize_text(s):\n    \"\"\"\n    Normalize the given text by lowercasing it, removing punctuation, articles, and extra whitespace.\n\n    Args:\n        s (str): The text to be normalized.\n\n    Returns:\n        str: The normalized text.\n    \"\"\"\n\n    def remove_articles(text):\n        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n\n    def white_space_fix(text):\n        return \" \".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \"\".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n</code></pre>"},{"location":"reference/models/hf/","title":"Transformers","text":""},{"location":"reference/models/hf/#ragfit.models.hf.HFInference","title":"<code>HFInference</code>","text":"<p>Class for running HF model inference locally.</p> Source code in <code>ragfit/models/hf.py</code> <pre><code>class HFInference:\n    \"\"\"\n    Class for running HF model inference locally.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        torch_dtype,\n        device_map,\n        instruction: Path,\n        instruct_in_prompt: False,\n        template: Path = None,\n        lora_path=None,\n        generation=None,\n        task=\"text-generation\",\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize a HF model, with optional LORA adapter.\n\n        Args:\n            model_name_or_path (str): HF model name or path.\n            torch_dtype (str): torch dtype for the model.\n            device_map: device map for the model.\n            instruction (Path): path to the instruction file.\n            instruct_in_prompt (bool): whether to include the instruction in the prompt for models without system role.\n            template (Path): path to a prompt template file if tokenizer does not include chat template. Optional.\n            lora_path (Path): path to the LORA adapter.\n            generation (dict): generation kwargs.\n            task (str): task for the pipeline.\n        \"\"\"\n\n        self.model_name = model_name_or_path\n        self.generation_kwargs = generation\n        self.instruction = open(instruction).read()\n        logger.info(f\"Using the following instruction: {self.instruction}\")\n\n        self.instruct_in_prompt = instruct_in_prompt\n        self.template = open(template).read() if template else None\n\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, **kwargs)\n\n        self.config = AutoConfig.from_pretrained(self.model_name, **kwargs)\n        self.config.torch_dtype = torch_dtype or \"auto\"\n\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name, config=self.config, device_map=device_map, **kwargs\n        )\n        if lora_path:\n            logger.info(f\"Loading LORA: {lora_path}\")\n            self.model.load_adapter(lora_path)\n\n        self.pipe = pipeline(\n            task=task,\n            model=self.model,\n            tokenizer=self.tokenizer,\n        )\n\n    def generate(self, prompt: str) -&gt; str:\n        \"\"\"\n        Given an input, generate a response.\n        \"\"\"\n\n        if self.template:\n            prompt = self.template.format(instruction=self.instruction, query=prompt)\n\n        else:\n            if self.instruct_in_prompt:\n                prompt = self.instruction + \"\\n\" + prompt\n\n            messages = [\n                {\"role\": \"system\", \"content\": self.instruction},\n                {\"role\": \"user\", \"content\": prompt},\n            ]\n\n            prompt = self.tokenizer.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True,\n                truncation=True,\n                max_length=(\n                    self.config.max_position_embeddings\n                    - self.generation_kwargs[\"max_new_tokens\"]\n                ),\n            )\n\n        output = self.pipe(prompt, **self.generation_kwargs)\n        return output[0][\"generated_text\"]\n</code></pre>"},{"location":"reference/models/hf/#ragfit.models.hf.HFInference.__init__","title":"<code>__init__(model_name_or_path: str, torch_dtype, device_map, instruction: Path, instruct_in_prompt: False, template: Path = None, lora_path=None, generation=None, task='text-generation', **kwargs)</code>","text":"<p>Initialize a HF model, with optional LORA adapter.</p> <p>Parameters:</p> <ul> <li> <code>model_name_or_path</code>               (<code>str</code>)           \u2013            <p>HF model name or path.</p> </li> <li> <code>torch_dtype</code>               (<code>str</code>)           \u2013            <p>torch dtype for the model.</p> </li> <li> <code>device_map</code>           \u2013            <p>device map for the model.</p> </li> <li> <code>instruction</code>               (<code>Path</code>)           \u2013            <p>path to the instruction file.</p> </li> <li> <code>instruct_in_prompt</code>               (<code>bool</code>)           \u2013            <p>whether to include the instruction in the prompt for models without system role.</p> </li> <li> <code>template</code>               (<code>Path</code>, default:                   <code>None</code> )           \u2013            <p>path to a prompt template file if tokenizer does not include chat template. Optional.</p> </li> <li> <code>lora_path</code>               (<code>Path</code>, default:                   <code>None</code> )           \u2013            <p>path to the LORA adapter.</p> </li> <li> <code>generation</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>generation kwargs.</p> </li> <li> <code>task</code>               (<code>str</code>, default:                   <code>'text-generation'</code> )           \u2013            <p>task for the pipeline.</p> </li> </ul> Source code in <code>ragfit/models/hf.py</code> <pre><code>def __init__(\n    self,\n    model_name_or_path: str,\n    torch_dtype,\n    device_map,\n    instruction: Path,\n    instruct_in_prompt: False,\n    template: Path = None,\n    lora_path=None,\n    generation=None,\n    task=\"text-generation\",\n    **kwargs,\n):\n    \"\"\"\n    Initialize a HF model, with optional LORA adapter.\n\n    Args:\n        model_name_or_path (str): HF model name or path.\n        torch_dtype (str): torch dtype for the model.\n        device_map: device map for the model.\n        instruction (Path): path to the instruction file.\n        instruct_in_prompt (bool): whether to include the instruction in the prompt for models without system role.\n        template (Path): path to a prompt template file if tokenizer does not include chat template. Optional.\n        lora_path (Path): path to the LORA adapter.\n        generation (dict): generation kwargs.\n        task (str): task for the pipeline.\n    \"\"\"\n\n    self.model_name = model_name_or_path\n    self.generation_kwargs = generation\n    self.instruction = open(instruction).read()\n    logger.info(f\"Using the following instruction: {self.instruction}\")\n\n    self.instruct_in_prompt = instruct_in_prompt\n    self.template = open(template).read() if template else None\n\n    self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, **kwargs)\n\n    self.config = AutoConfig.from_pretrained(self.model_name, **kwargs)\n    self.config.torch_dtype = torch_dtype or \"auto\"\n\n    self.model = AutoModelForCausalLM.from_pretrained(\n        self.model_name, config=self.config, device_map=device_map, **kwargs\n    )\n    if lora_path:\n        logger.info(f\"Loading LORA: {lora_path}\")\n        self.model.load_adapter(lora_path)\n\n    self.pipe = pipeline(\n        task=task,\n        model=self.model,\n        tokenizer=self.tokenizer,\n    )\n</code></pre>"},{"location":"reference/models/hf/#ragfit.models.hf.HFInference.generate","title":"<code>generate(prompt: str) -&gt; str</code>","text":"<p>Given an input, generate a response.</p> Source code in <code>ragfit/models/hf.py</code> <pre><code>def generate(self, prompt: str) -&gt; str:\n    \"\"\"\n    Given an input, generate a response.\n    \"\"\"\n\n    if self.template:\n        prompt = self.template.format(instruction=self.instruction, query=prompt)\n\n    else:\n        if self.instruct_in_prompt:\n            prompt = self.instruction + \"\\n\" + prompt\n\n        messages = [\n            {\"role\": \"system\", \"content\": self.instruction},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        prompt = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True,\n            truncation=True,\n            max_length=(\n                self.config.max_position_embeddings\n                - self.generation_kwargs[\"max_new_tokens\"]\n            ),\n        )\n\n    output = self.pipe(prompt, **self.generation_kwargs)\n    return output[0][\"generated_text\"]\n</code></pre>"},{"location":"reference/models/hf/#ragfit.models.hf.HFTrain","title":"<code>HFTrain</code>","text":"<p>Class for training HF models locally.</p> Source code in <code>ragfit/models/hf.py</code> <pre><code>class HFTrain:\n    \"\"\"\n    Class for training HF models locally.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path,\n        torch_dtype,\n        device_map,\n        lora: LoraConfig = None,\n        generation=None,\n        completion_start: str = \"\",\n        instruction_in_prompt=None,\n        max_sequence_len=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            model_name_or_path: str - HF model name or path.\n            torch_dtype: str - torch dtype for the model.\n            device_map: dict - device map for the model.\n            lora: dict - LoRA adapter config.\n            generation: dict - generation kwargs.\n            completion_start: str - used to find the start of the completion in the prompt.\n            instruction_in_prompt: bool - whether to include the instruction in the prompt for models without system role.\n        \"\"\"\n        self.model_name = model_name_or_path\n        self.complete_start = completion_start\n        self.instructions_in_prompt = instruction_in_prompt\n        self.generation_kwargs = generation\n\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        self.config = AutoConfig.from_pretrained(self.model_name, **kwargs)\n        self.config.torch_dtype = torch_dtype or \"auto\"\n\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name,\n            config=self.config,\n            device_map=device_map,\n            **kwargs,\n        )\n\n        self.model.config.use_cache = False\n        logger.info(f\"Loaded model: {self.model}\")\n\n        logger.info(f\"Initializing LORA based on {lora}\")\n        self.model = get_peft_model(self.model, LoraConfig(**lora))\n</code></pre>"},{"location":"reference/models/hf/#ragfit.models.hf.HFTrain.__init__","title":"<code>__init__(model_name_or_path, torch_dtype, device_map, lora: LoraConfig = None, generation=None, completion_start: str = '', instruction_in_prompt=None, max_sequence_len=None, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>model_name_or_path</code>           \u2013            <p>str - HF model name or path.</p> </li> <li> <code>torch_dtype</code>           \u2013            <p>str - torch dtype for the model.</p> </li> <li> <code>device_map</code>           \u2013            <p>dict - device map for the model.</p> </li> <li> <code>lora</code>               (<code>LoraConfig</code>, default:                   <code>None</code> )           \u2013            <p>dict - LoRA adapter config.</p> </li> <li> <code>generation</code>           \u2013            <p>dict - generation kwargs.</p> </li> <li> <code>completion_start</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>str - used to find the start of the completion in the prompt.</p> </li> <li> <code>instruction_in_prompt</code>           \u2013            <p>bool - whether to include the instruction in the prompt for models without system role.</p> </li> </ul> Source code in <code>ragfit/models/hf.py</code> <pre><code>def __init__(\n    self,\n    model_name_or_path,\n    torch_dtype,\n    device_map,\n    lora: LoraConfig = None,\n    generation=None,\n    completion_start: str = \"\",\n    instruction_in_prompt=None,\n    max_sequence_len=None,\n    **kwargs,\n):\n    \"\"\"\n    Args:\n        model_name_or_path: str - HF model name or path.\n        torch_dtype: str - torch dtype for the model.\n        device_map: dict - device map for the model.\n        lora: dict - LoRA adapter config.\n        generation: dict - generation kwargs.\n        completion_start: str - used to find the start of the completion in the prompt.\n        instruction_in_prompt: bool - whether to include the instruction in the prompt for models without system role.\n    \"\"\"\n    self.model_name = model_name_or_path\n    self.complete_start = completion_start\n    self.instructions_in_prompt = instruction_in_prompt\n    self.generation_kwargs = generation\n\n    self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n    self.tokenizer.pad_token = self.tokenizer.eos_token\n\n    self.config = AutoConfig.from_pretrained(self.model_name, **kwargs)\n    self.config.torch_dtype = torch_dtype or \"auto\"\n\n    self.model = AutoModelForCausalLM.from_pretrained(\n        self.model_name,\n        config=self.config,\n        device_map=device_map,\n        **kwargs,\n    )\n\n    self.model.config.use_cache = False\n    logger.info(f\"Loaded model: {self.model}\")\n\n    logger.info(f\"Initializing LORA based on {lora}\")\n    self.model = get_peft_model(self.model, LoraConfig(**lora))\n</code></pre>"},{"location":"reference/models/openai_executor/","title":"OpenAI","text":""},{"location":"reference/models/openai_executor/#ragfit.models.openai_executor.OpenAIExecutor","title":"<code>OpenAIExecutor</code>","text":"<p>Class representing an interface to the Azure OpenAI API.</p> Source code in <code>ragfit/models/openai_executor.py</code> <pre><code>class OpenAIExecutor:\n    \"\"\"\n    Class representing an interface to the Azure OpenAI API.\n    \"\"\"\n\n    def __init__(\n        self,\n        azure_endpoint: str,\n        api_key: str = None,\n        api_version: str = \"2024-02-15-preview\",\n        model: str = \"GPT-4-32k-Bot\",\n        chat_parameters: dict = None,\n        delay: int = 1,\n    ):\n        \"\"\"\n        Initialize the OpenAIExecutor.\n\n        Args:\n            azure_endpoint (str): The Azure endpoint.\n            api_key (str): The API key, can also read of ENV variable.\n            api_version (str): The API version.\n            model (str): The model to use, sometimes called deployment or engine.\n            chat_parameters (dict): The chat parameters.\n            delay (int): delay between calls.\n        \"\"\"\n        self.delay = delay\n        self.model = model\n        self.chat_parameters = dict(\n            temperature=0.7,\n            max_tokens=200,\n            top_p=0.95,\n            frequency_penalty=0,\n            presence_penalty=0,\n            stop=None,\n        )\n        if chat_parameters:\n            self.chat_parameters.update(chat_parameters)\n\n        self.client = AzureOpenAI(\n            azure_endpoint=azure_endpoint,\n            api_key=api_key or os.getenv(\"AZURE_OPENAI_API_KEY\"),\n            api_version=api_version,\n        )\n\n    def chat(self, prompt: Union[List, str], instruction: str = None) -&gt; str:\n        \"\"\"\n        Chat with the OpenAI API.\n\n        Args:\n            prompt (Union[List, str]): The prompt to chat.\n            instruction (str): The instruction to use.\n\n        Returns:\n            str: The response. Empty string if error.\n        \"\"\"\n        if isinstance(prompt, str):\n            prompt = [\n                {\n                    \"role\": \"system\",\n                    \"content\": (\n                        instruction\n                        or \"You are an AI assistant that helps people find information.\"\n                    ),\n                },\n                {\"role\": \"user\", \"content\": prompt},\n            ]\n\n        if self.delay:\n            time.sleep(self.delay)\n\n        try:\n            completion = self.client.chat.completions.create(\n                model=self.model,\n                messages=prompt,\n                **self.chat_parameters,\n            )\n            message_obj = completion.choices[0].message\n\n            if hasattr(message_obj, \"content\"):\n                answer = message_obj.content\n                return answer or \"\"\n            else:\n                return \"\"\n\n        except Exception as e:\n            logging.info(f\"OPENAI error:\\n{e}\")\n            return \"\"\n</code></pre>"},{"location":"reference/models/openai_executor/#ragfit.models.openai_executor.OpenAIExecutor.__init__","title":"<code>__init__(azure_endpoint: str, api_key: str = None, api_version: str = '2024-02-15-preview', model: str = 'GPT-4-32k-Bot', chat_parameters: dict = None, delay: int = 1)</code>","text":"<p>Initialize the OpenAIExecutor.</p> <p>Parameters:</p> <ul> <li> <code>azure_endpoint</code>               (<code>str</code>)           \u2013            <p>The Azure endpoint.</p> </li> <li> <code>api_key</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The API key, can also read of ENV variable.</p> </li> <li> <code>api_version</code>               (<code>str</code>, default:                   <code>'2024-02-15-preview'</code> )           \u2013            <p>The API version.</p> </li> <li> <code>model</code>               (<code>str</code>, default:                   <code>'GPT-4-32k-Bot'</code> )           \u2013            <p>The model to use, sometimes called deployment or engine.</p> </li> <li> <code>chat_parameters</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>The chat parameters.</p> </li> <li> <code>delay</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>delay between calls.</p> </li> </ul> Source code in <code>ragfit/models/openai_executor.py</code> <pre><code>def __init__(\n    self,\n    azure_endpoint: str,\n    api_key: str = None,\n    api_version: str = \"2024-02-15-preview\",\n    model: str = \"GPT-4-32k-Bot\",\n    chat_parameters: dict = None,\n    delay: int = 1,\n):\n    \"\"\"\n    Initialize the OpenAIExecutor.\n\n    Args:\n        azure_endpoint (str): The Azure endpoint.\n        api_key (str): The API key, can also read of ENV variable.\n        api_version (str): The API version.\n        model (str): The model to use, sometimes called deployment or engine.\n        chat_parameters (dict): The chat parameters.\n        delay (int): delay between calls.\n    \"\"\"\n    self.delay = delay\n    self.model = model\n    self.chat_parameters = dict(\n        temperature=0.7,\n        max_tokens=200,\n        top_p=0.95,\n        frequency_penalty=0,\n        presence_penalty=0,\n        stop=None,\n    )\n    if chat_parameters:\n        self.chat_parameters.update(chat_parameters)\n\n    self.client = AzureOpenAI(\n        azure_endpoint=azure_endpoint,\n        api_key=api_key or os.getenv(\"AZURE_OPENAI_API_KEY\"),\n        api_version=api_version,\n    )\n</code></pre>"},{"location":"reference/models/openai_executor/#ragfit.models.openai_executor.OpenAIExecutor.chat","title":"<code>chat(prompt: Union[List, str], instruction: str = None) -&gt; str</code>","text":"<p>Chat with the OpenAI API.</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>               (<code>Union[List, str]</code>)           \u2013            <p>The prompt to chat.</p> </li> <li> <code>instruction</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The instruction to use.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The response. Empty string if error.</p> </li> </ul> Source code in <code>ragfit/models/openai_executor.py</code> <pre><code>def chat(self, prompt: Union[List, str], instruction: str = None) -&gt; str:\n    \"\"\"\n    Chat with the OpenAI API.\n\n    Args:\n        prompt (Union[List, str]): The prompt to chat.\n        instruction (str): The instruction to use.\n\n    Returns:\n        str: The response. Empty string if error.\n    \"\"\"\n    if isinstance(prompt, str):\n        prompt = [\n            {\n                \"role\": \"system\",\n                \"content\": (\n                    instruction\n                    or \"You are an AI assistant that helps people find information.\"\n                ),\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n    if self.delay:\n        time.sleep(self.delay)\n\n    try:\n        completion = self.client.chat.completions.create(\n            model=self.model,\n            messages=prompt,\n            **self.chat_parameters,\n        )\n        message_obj = completion.choices[0].message\n\n        if hasattr(message_obj, \"content\"):\n            answer = message_obj.content\n            return answer or \"\"\n        else:\n            return \"\"\n\n    except Exception as e:\n        logging.info(f\"OPENAI error:\\n{e}\")\n        return \"\"\n</code></pre>"},{"location":"reference/models/vllm/","title":"vLLM","text":""},{"location":"reference/models/vllm/#ragfit.models.vllm.VLLMInference","title":"<code>VLLMInference</code>","text":"<p>Initializes a vLLM-based inference engine.</p> <p>Parameters:</p> <ul> <li> <code>model_name_or_path</code>               (<code>str</code>)           \u2013            <p>The name or path of the model.</p> </li> <li> <code>instruction</code>               (<code>Path</code>)           \u2013            <p>path to the instruction file.</p> </li> <li> <code>instruct_in_prompt</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>whether to include the instruction in the prompt for models without system role.</p> </li> <li> <code>template</code>               (<code>Path</code>, default:                   <code>None</code> )           \u2013            <p>path to a prompt template file if tokenizer does not include chat template. Optional.</p> </li> <li> <code>num_gpus</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of GPUs to use. Defaults to 1.</p> </li> <li> <code>llm_params</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional parameters for the LLM model. Supports all parameters define by vLLM LLM engine. Defaults to an empty dictionary.</p> </li> <li> <code>generation</code>               (<code>Dict</code>, default:                   <code>{}</code> )           \u2013            <p>Additional parameters for text generation. Supports all the keywords of <code>SamplingParams</code> of vLLM. Defaults to an empty dictionary.</p> </li> </ul> Source code in <code>ragfit/models/vllm.py</code> <pre><code>class VLLMInference:\n    \"\"\"\n    Initializes a vLLM-based inference engine.\n\n    Args:\n        model_name_or_path (str): The name or path of the model.\n        instruction (Path): path to the instruction file.\n        instruct_in_prompt (bool): whether to include the instruction in the prompt for models without system role.\n        template (Path): path to a prompt template file if tokenizer does not include chat template. Optional.\n        num_gpus (int, optional): The number of GPUs to use. Defaults to 1.\n        llm_params (Dict, optional): Additional parameters for the LLM model. Supports all parameters define by vLLM LLM engine. Defaults to an empty dictionary.\n        generation (Dict, optional): Additional parameters for text generation. Supports all the keywords of `SamplingParams` of vLLM. Defaults to an empty dictionary.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name_or_path: str,\n        instruction: Path,\n        instruct_in_prompt: bool = False,\n        template: Path = None,\n        num_gpus: int = 1,\n        llm_params: Dict = {},\n        generation: Dict = {},\n    ):\n        check_package_installed(\n            \"vllm\",\n            \"please refer to vLLM website for installation instructions, or run: pip install vllm\",\n        )\n        from vllm import LLM, SamplingParams\n\n        self.model_name = model_name_or_path\n        self.instruct_in_prompt = instruct_in_prompt\n        self.template = open(template).read() if template else None\n        self.instruction = open(instruction).read()\n        logger.info(f\"Using the following instruction: {self.instruction}\")\n\n        self.sampling_params = SamplingParams(**generation)\n        self.llm = LLM(model=self.model_name, tensor_parallel_size=num_gpus, **llm_params)\n\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.config = AutoConfig.from_pretrained(self.model_name)\n\n    def generate(self, prompt: str) -&gt; str:\n        \"\"\"\n        Generates text based on the given prompt.\n        \"\"\"\n        if self.template:\n            prompt = self.template.format(instruction=self.instruction, query=prompt)\n        else:\n            if self.instruct_in_prompt:\n                prompt = self.instruction + \"\\n\" + prompt\n            messages = [\n                {\"role\": \"system\", \"content\": self.instruction},\n                {\"role\": \"user\", \"content\": prompt},\n            ]\n\n            prompt = self.tokenizer.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True,\n                truncation=True,\n                max_length=(\n                    self.config.max_position_embeddings - self.sampling_params.max_tokens\n                ),\n            )\n\n        output = self.llm.generate(prompt, self.sampling_params)\n        return output[0].outputs[0].text\n</code></pre>"},{"location":"reference/models/vllm/#ragfit.models.vllm.VLLMInference.generate","title":"<code>generate(prompt: str) -&gt; str</code>","text":"<p>Generates text based on the given prompt.</p> Source code in <code>ragfit/models/vllm.py</code> <pre><code>def generate(self, prompt: str) -&gt; str:\n    \"\"\"\n    Generates text based on the given prompt.\n    \"\"\"\n    if self.template:\n        prompt = self.template.format(instruction=self.instruction, query=prompt)\n    else:\n        if self.instruct_in_prompt:\n            prompt = self.instruction + \"\\n\" + prompt\n        messages = [\n            {\"role\": \"system\", \"content\": self.instruction},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        prompt = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True,\n            truncation=True,\n            max_length=(\n                self.config.max_position_embeddings - self.sampling_params.max_tokens\n            ),\n        )\n\n    output = self.llm.generate(prompt, self.sampling_params)\n    return output[0].outputs[0].text\n</code></pre>"},{"location":"reference/processing/pipeline/","title":"Pipeline","text":""},{"location":"reference/processing/pipeline/#ragfit.processing.pipeline.DataPipeline","title":"<code>DataPipeline</code>","text":"<p>Class for creating a data pipeline.</p> <p>The pipeline holds the list of steps and run them one after the other. The datasets are stored in a global dictionary, where datasets are referred by a key name, as indicated in the <code>inputs</code> parameter for each step. The pipeline manages the cache lookup and creation.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name of the pipeline.</p> </li> <li> <code>output_path</code>               (<code>str</code>, default:                   <code>'.'</code> )           \u2013            <p>Path to store the cache files. Defaults to \".\".</p> </li> <li> <code>cache</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to cache the datasets. Defaults to True.</p> </li> <li> <code>steps</code>               (<code>List[BaseStep]</code>, default:                   <code>[]</code> )           \u2013            <p>List of steps in the pipeline. Defaults to [].</p> </li> <li> <code>inputs</code>               (<code>str</code>, default:                   <code>'main_dataset'</code> )           \u2013            <p>Name of the main dataset. Defaults to \"main_dataset\".</p> </li> </ul> Source code in <code>ragfit/processing/pipeline.py</code> <pre><code>class DataPipeline:\n    \"\"\"Class for creating a data pipeline.\n\n    The pipeline holds the list of steps and run them one after the other. The\n    datasets are stored in a global dictionary, where datasets are referred by a\n    key name, as indicated in the `inputs` parameter for each step. The pipeline\n    manages the cache lookup and creation.\n\n    Args:\n        name (str): Name of the pipeline.\n        output_path (str, optional): Path to store the cache files. Defaults to \".\".\n        cache (bool, optional): Whether to cache the datasets. Defaults to True.\n        steps (List[BaseStep], optional): List of steps in the pipeline. Defaults to [].\n        inputs (str, optional): Name of the main dataset. Defaults to \"main_dataset\".\n\n    \"\"\"\n\n    def __init__(\n        self,\n        name,\n        output_path=\".\",\n        cache=True,\n        steps: List[BaseStep] = [],\n        inputs: str = \"main_dataset\",\n        **kwargs,\n    ) -&gt; None:\n        self.name = name\n        self.output_path = output_path\n        self.cache = cache\n        logging.info(f\"Caching state: {self.cache}\")\n        self.last_update = math.inf\n\n        self.steps = [\n            hydra.utils.instantiate(step, _convert_=\"object\") for step in steps\n        ]  # TODO: do it lazily to prevent OOM\n\n        self.inputs = inputs if isinstance(inputs, list) else [inputs]\n        self.datasets = {}\n\n    def gen_cache_fn(self, step, index, dataset_name):\n        \"\"\"\n        Create a unique cache filename for  a given dataset, at a given step, in a given index.\n        Uses the step name, inputs, hash and pipeline's path and name and dataset name.\n\n        Returns a string.\n        \"\"\"\n        return (\n            f\"{self.output_path}/cache\"\n            f\"_{self.name}_{index}\"\n            f\"_{type(step).__name__}\"\n            f\"_{dataset_name}_{step.get_hash()}.json\"\n        )\n\n    def get_cache_mapping(self, step: BaseStep, index: int):\n        \"\"\"\n        Returns a mapping between input datasets and cache filenames, for a given step.\n        \"\"\"\n        if self.cache:\n            datasets_caches = {\n                dataset_name: self.gen_cache_fn(step, index, dataset_name)\n                for dataset_name in step.inputs\n            }\n            return datasets_caches\n\n        return None\n\n    def cache_step(self, step, step_index):\n        \"\"\"\n        Write to cache-files the current state of the global datasets dictionary for the given inputs.\n        \"\"\"\n        if self.cache:\n            for dataset_name in step.inputs:\n                dataset = self.datasets[dataset_name]\n                saved_path = self.gen_cache_fn(step, step_index, dataset_name)\n                dataset.to_json(saved_path, lines=True)\n\n    def load_from_cache(self, caches_map):\n        \"\"\"\n        Load datasets from cache using a cache_map.\n        Updates the global datasets dictionary.\n\n        Internal function, shouldn't be used by the user.\n        \"\"\"\n        logging.info(f\"Loading dataset from checkpoints {caches_map}\")\n        for dataset_name, saved_path in caches_map.items():\n            self.datasets[dataset_name] = load_dataset(\n                \"json\", data_files=[saved_path], split=\"train\"\n            )\n\n    def delete_cache(self):\n        \"\"\"\n        Removing cache files for all steps, cleaning the pipeline.\n        \"\"\"\n        logging.info(\"Removing cache files for entire pipeline.\")\n        if self.cache:\n            for i, step in enumerate(self.steps):\n                cache_map = self.get_cache_mapping(step, i)\n                if cache_map is not None:\n                    for dataset_name, cache_path in cache_map.items():\n                        if os.path.exists(cache_path):\n                            os.remove(cache_path)\n\n    def process(self):\n        \"\"\"\n        Run pipeline, step after step.\n\n        Caching is handled here. A step is calculated either if there was a change in the pipeline at a previous\n        step OR the current step has no cache file.\n\n        When a step is calculated, it is cached and self.last_update is updated to the current step index.\n        \"\"\"\n        for i, step in tqdm(enumerate(self.steps)):\n            logging.info(f\"Processing step {i}\")\n\n            cache_map = self.get_cache_mapping(step, i)\n            if (\n                (cache_map is not None)\n                and (all(os.path.exists(v) for v in cache_map.values()))\n                and (i &lt; self.last_update)\n            ):\n                logging.info(f\"Loading cached datasets for {type(step).__name__}\")\n                self.load_from_cache(cache_map)\n            else:\n                step(self.datasets)\n                if step.cache_step:\n                    self.cache_step(step, i)\n                    self.last_update = i\n</code></pre>"},{"location":"reference/processing/pipeline/#ragfit.processing.pipeline.DataPipeline.cache_step","title":"<code>cache_step(step, step_index)</code>","text":"<p>Write to cache-files the current state of the global datasets dictionary for the given inputs.</p> Source code in <code>ragfit/processing/pipeline.py</code> <pre><code>def cache_step(self, step, step_index):\n    \"\"\"\n    Write to cache-files the current state of the global datasets dictionary for the given inputs.\n    \"\"\"\n    if self.cache:\n        for dataset_name in step.inputs:\n            dataset = self.datasets[dataset_name]\n            saved_path = self.gen_cache_fn(step, step_index, dataset_name)\n            dataset.to_json(saved_path, lines=True)\n</code></pre>"},{"location":"reference/processing/pipeline/#ragfit.processing.pipeline.DataPipeline.delete_cache","title":"<code>delete_cache()</code>","text":"<p>Removing cache files for all steps, cleaning the pipeline.</p> Source code in <code>ragfit/processing/pipeline.py</code> <pre><code>def delete_cache(self):\n    \"\"\"\n    Removing cache files for all steps, cleaning the pipeline.\n    \"\"\"\n    logging.info(\"Removing cache files for entire pipeline.\")\n    if self.cache:\n        for i, step in enumerate(self.steps):\n            cache_map = self.get_cache_mapping(step, i)\n            if cache_map is not None:\n                for dataset_name, cache_path in cache_map.items():\n                    if os.path.exists(cache_path):\n                        os.remove(cache_path)\n</code></pre>"},{"location":"reference/processing/pipeline/#ragfit.processing.pipeline.DataPipeline.gen_cache_fn","title":"<code>gen_cache_fn(step, index, dataset_name)</code>","text":"<p>Create a unique cache filename for  a given dataset, at a given step, in a given index. Uses the step name, inputs, hash and pipeline's path and name and dataset name.</p> <p>Returns a string.</p> Source code in <code>ragfit/processing/pipeline.py</code> <pre><code>def gen_cache_fn(self, step, index, dataset_name):\n    \"\"\"\n    Create a unique cache filename for  a given dataset, at a given step, in a given index.\n    Uses the step name, inputs, hash and pipeline's path and name and dataset name.\n\n    Returns a string.\n    \"\"\"\n    return (\n        f\"{self.output_path}/cache\"\n        f\"_{self.name}_{index}\"\n        f\"_{type(step).__name__}\"\n        f\"_{dataset_name}_{step.get_hash()}.json\"\n    )\n</code></pre>"},{"location":"reference/processing/pipeline/#ragfit.processing.pipeline.DataPipeline.get_cache_mapping","title":"<code>get_cache_mapping(step: BaseStep, index: int)</code>","text":"<p>Returns a mapping between input datasets and cache filenames, for a given step.</p> Source code in <code>ragfit/processing/pipeline.py</code> <pre><code>def get_cache_mapping(self, step: BaseStep, index: int):\n    \"\"\"\n    Returns a mapping between input datasets and cache filenames, for a given step.\n    \"\"\"\n    if self.cache:\n        datasets_caches = {\n            dataset_name: self.gen_cache_fn(step, index, dataset_name)\n            for dataset_name in step.inputs\n        }\n        return datasets_caches\n\n    return None\n</code></pre>"},{"location":"reference/processing/pipeline/#ragfit.processing.pipeline.DataPipeline.load_from_cache","title":"<code>load_from_cache(caches_map)</code>","text":"<p>Load datasets from cache using a cache_map. Updates the global datasets dictionary.</p> <p>Internal function, shouldn't be used by the user.</p> Source code in <code>ragfit/processing/pipeline.py</code> <pre><code>def load_from_cache(self, caches_map):\n    \"\"\"\n    Load datasets from cache using a cache_map.\n    Updates the global datasets dictionary.\n\n    Internal function, shouldn't be used by the user.\n    \"\"\"\n    logging.info(f\"Loading dataset from checkpoints {caches_map}\")\n    for dataset_name, saved_path in caches_map.items():\n        self.datasets[dataset_name] = load_dataset(\n            \"json\", data_files=[saved_path], split=\"train\"\n        )\n</code></pre>"},{"location":"reference/processing/pipeline/#ragfit.processing.pipeline.DataPipeline.process","title":"<code>process()</code>","text":"<p>Run pipeline, step after step.</p> <p>Caching is handled here. A step is calculated either if there was a change in the pipeline at a previous step OR the current step has no cache file.</p> <p>When a step is calculated, it is cached and self.last_update is updated to the current step index.</p> Source code in <code>ragfit/processing/pipeline.py</code> <pre><code>def process(self):\n    \"\"\"\n    Run pipeline, step after step.\n\n    Caching is handled here. A step is calculated either if there was a change in the pipeline at a previous\n    step OR the current step has no cache file.\n\n    When a step is calculated, it is cached and self.last_update is updated to the current step index.\n    \"\"\"\n    for i, step in tqdm(enumerate(self.steps)):\n        logging.info(f\"Processing step {i}\")\n\n        cache_map = self.get_cache_mapping(step, i)\n        if (\n            (cache_map is not None)\n            and (all(os.path.exists(v) for v in cache_map.values()))\n            and (i &lt; self.last_update)\n        ):\n            logging.info(f\"Loading cached datasets for {type(step).__name__}\")\n            self.load_from_cache(cache_map)\n        else:\n            step(self.datasets)\n            if step.cache_step:\n                self.cache_step(step, i)\n                self.last_update = i\n</code></pre>"},{"location":"reference/processing/step/","title":"Step","text":""},{"location":"reference/processing/step/#ragfit.processing.step.BaseStep","title":"<code>BaseStep</code>","text":"<p>Class representing a step in a processing pipeline. Entry point is <code>__call__</code>. Users would inherit either LocalStep or GlobalStep.</p> <p>Step can be cached (on by default: <code>cache_step=True</code>) to prevent re-computation.</p> <p>Individual steps can disable caching if and only if they do not manipulate the dataset, as re-computation of later steps is conditioned on the necessity of caching.</p> Source code in <code>ragfit/processing/step.py</code> <pre><code>class BaseStep:\n    \"\"\"\n    Class representing a step in a processing pipeline.\n    Entry point is `__call__`.\n    Users would inherit either LocalStep or GlobalStep.\n\n    Step can be cached (on by default: `cache_step=True`) to prevent re-computation.\n\n    Individual steps can disable caching if and only if they do not manipulate the dataset, as\n    re-computation of later steps is conditioned on the necessity of caching.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        self.kwargs = kwargs\n        self.inputs: list[str] = kwargs.get(\"inputs\", [\"main_dataset\"])\n        self.step_hash = None\n        self.cache_step = True\n\n        if isinstance(self.inputs, str):\n            self.inputs = [self.inputs]\n\n        assert (\n            not isinstance(self.inputs, str) and len(self.inputs) &gt; 0\n        ), f\"`inputs` should be a list, got {type(self.inputs)}\"\n\n    def calc_hash(self):\n        \"\"\"\n        Calculate hash for a step based on its properties.\n        Updates the `step_hash` property.\n        \"\"\"\n        args_to_hash = {}\n        for property, value in vars(self).items():\n            if is_jsonable(value):\n                args_to_hash[property] = value\n        self.step_hash = dict_hash(args_to_hash)\n\n    def get_hash(self):\n        \"\"\"\n        Step hash getter. If hash is not calculated, it calculates it first.\n        \"\"\"\n        if self.step_hash is None:\n            self.calc_hash()\n        return self.step_hash\n\n    def __call__(self, datasets, **kwargs):\n        \"\"\"\n        Pipeline is running these steps using `__call__`.\n        \"\"\"\n        logging.info(f\"Running processing step: {type(self).__name__}\")\n        self.process_inputs(datasets, **kwargs)\n\n    def process_inputs(self, datasets, **kwargs):\n        \"\"\"\n        Run the step `process` function for each dataset in `inputs`.\n        \"\"\"\n        for dataset_name in self.inputs:\n            self.process(dataset_name, datasets, **kwargs)\n\n    def process(self, dataset_name, datasets, **kwargs):\n        \"\"\"\n        General processing of `dataset_name` in `datasets`, in place.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/processing/step/#ragfit.processing.step.BaseStep.__call__","title":"<code>__call__(datasets, **kwargs)</code>","text":"<p>Pipeline is running these steps using <code>__call__</code>.</p> Source code in <code>ragfit/processing/step.py</code> <pre><code>def __call__(self, datasets, **kwargs):\n    \"\"\"\n    Pipeline is running these steps using `__call__`.\n    \"\"\"\n    logging.info(f\"Running processing step: {type(self).__name__}\")\n    self.process_inputs(datasets, **kwargs)\n</code></pre>"},{"location":"reference/processing/step/#ragfit.processing.step.BaseStep.calc_hash","title":"<code>calc_hash()</code>","text":"<p>Calculate hash for a step based on its properties. Updates the <code>step_hash</code> property.</p> Source code in <code>ragfit/processing/step.py</code> <pre><code>def calc_hash(self):\n    \"\"\"\n    Calculate hash for a step based on its properties.\n    Updates the `step_hash` property.\n    \"\"\"\n    args_to_hash = {}\n    for property, value in vars(self).items():\n        if is_jsonable(value):\n            args_to_hash[property] = value\n    self.step_hash = dict_hash(args_to_hash)\n</code></pre>"},{"location":"reference/processing/step/#ragfit.processing.step.BaseStep.get_hash","title":"<code>get_hash()</code>","text":"<p>Step hash getter. If hash is not calculated, it calculates it first.</p> Source code in <code>ragfit/processing/step.py</code> <pre><code>def get_hash(self):\n    \"\"\"\n    Step hash getter. If hash is not calculated, it calculates it first.\n    \"\"\"\n    if self.step_hash is None:\n        self.calc_hash()\n    return self.step_hash\n</code></pre>"},{"location":"reference/processing/step/#ragfit.processing.step.BaseStep.process","title":"<code>process(dataset_name, datasets, **kwargs)</code>","text":"<p>General processing of <code>dataset_name</code> in <code>datasets</code>, in place.</p> Source code in <code>ragfit/processing/step.py</code> <pre><code>def process(self, dataset_name, datasets, **kwargs):\n    \"\"\"\n    General processing of `dataset_name` in `datasets`, in place.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/processing/step/#ragfit.processing.step.BaseStep.process_inputs","title":"<code>process_inputs(datasets, **kwargs)</code>","text":"<p>Run the step <code>process</code> function for each dataset in <code>inputs</code>.</p> Source code in <code>ragfit/processing/step.py</code> <pre><code>def process_inputs(self, datasets, **kwargs):\n    \"\"\"\n    Run the step `process` function for each dataset in `inputs`.\n    \"\"\"\n    for dataset_name in self.inputs:\n        self.process(dataset_name, datasets, **kwargs)\n</code></pre>"},{"location":"reference/processing/step/#ragfit.processing.step.GlobalStep","title":"<code>GlobalStep</code>","text":"<p>               Bases: <code>BaseStep</code></p> <p>Class representing a step in a processing pipeline, processing the entire dataset.</p> <p>The function to overwrite is <code>process_all</code>; the function accepts the dataset and all the other datasets, if needed.</p> Source code in <code>ragfit/processing/step.py</code> <pre><code>class GlobalStep(BaseStep):\n    \"\"\"\n    Class representing a step in a processing pipeline, processing the entire dataset.\n\n    The function to overwrite is `process_all`; the function accepts the dataset and all the other datasets, if needed.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def process(self, dataset_name, datasets, **kwargs):\n        datasets[dataset_name] = self.process_all(\n            datasets[dataset_name], datasets, **kwargs\n        )\n\n    def process_all(self, dataset, datasets, **kwargs):\n        return dataset\n</code></pre>"},{"location":"reference/processing/step/#ragfit.processing.step.LocalStep","title":"<code>LocalStep</code>","text":"<p>               Bases: <code>BaseStep</code></p> <p>Class representing a step in a processing pipeline, processing individual examples.</p> <p>The function to overwrite is <code>process_item</code>; the function accepts an item, index, and all the other datasets, if needed.</p> Source code in <code>ragfit/processing/step.py</code> <pre><code>class LocalStep(BaseStep):\n    \"\"\"\n    Class representing a step in a processing pipeline, processing individual examples.\n\n    The function to overwrite is `process_item`; the function accepts an item, index, and all the other datasets, if needed.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def process(self, dataset_name, datasets, **kwargs):\n        datasets[dataset_name] = datasets[dataset_name].map(\n            lambda item, index: self.process_item(item, index, datasets, **kwargs),\n            with_indices=True,\n            load_from_cache_file=False,\n        )\n\n    def process_item(self, item, index, datasets, **kwargs):\n        return item\n</code></pre>"},{"location":"reference/processing/utils/","title":"Utils","text":""},{"location":"reference/processing/utils/#ragfit.processing.utils.dict_hash","title":"<code>dict_hash(dictionary: Dict[str, Any]) -&gt; str</code>","text":"<p>Hash dictionary using MD5. Used in step caching; steps are cached based on the signature.</p> Source code in <code>ragfit/processing/utils.py</code> <pre><code>def dict_hash(dictionary: Dict[str, Any]) -&gt; str:\n    \"\"\"\n    Hash dictionary using MD5. Used in step caching; steps are cached based on the signature.\n    \"\"\"\n    dhash = hashlib.md5()\n    encoded = json.dumps(dictionary, sort_keys=True).encode()\n    dhash.update(encoded)\n    return dhash.hexdigest()\n</code></pre>"},{"location":"reference/processing/utils/#ragfit.processing.utils.is_jsonable","title":"<code>is_jsonable(x)</code>","text":"<p>Test if input is JSON-serializable.</p> Source code in <code>ragfit/processing/utils.py</code> <pre><code>def is_jsonable(x):\n    \"\"\"\n    Test if input is JSON-serializable.\n    \"\"\"\n    try:\n        json.dumps(x)\n        return True\n    except (TypeError, OverflowError):\n        return False\n</code></pre>"},{"location":"reference/processing/answer_processors/regex/","title":"regex","text":""},{"location":"reference/processing/answer_processors/regex/#ragfit.processing.answer_processors.regex.RegexAnswer","title":"<code>RegexAnswer</code>","text":"<p>Extract answers from the text using regular expressions.</p> <p>Pattern is the regular expression used to extract the answer. Stopping pattern is a string used to split the answer.</p> <p>Example: <code>r = RegexAnswer(\"&lt;ANSWER&gt;: (.*)\", \"[,.;]\")</code></p> Source code in <code>ragfit/processing/answer_processors/regex.py</code> <pre><code>class RegexAnswer:\n    \"\"\"\n    Extract answers from the text using regular expressions.\n\n    Pattern is the regular expression used to extract the answer.\n    Stopping pattern is a string used to split the answer.\n\n    Example:\n    `r = RegexAnswer(\"&lt;ANSWER&gt;: (.*)\", \"[,.;]\")`\n    \"\"\"\n\n    def __init__(self, capture_pattern=None, stopping_pattern=None):\n        self.capture_pattern = capture_pattern\n        self.stopping_pattern = stopping_pattern\n\n    def __call__(self, text: str):\n        \"\"\"\n        Extract the answer from the text.\n        \"\"\"\n        if (capture := self.capture_pattern) and capture != \"\":\n            match = re.search(capture, text, re.MULTILINE | re.DOTALL)\n            if match:\n                text = match.group(1)\n\n        if (stopping := self.stopping_pattern) and stopping != \"\":\n            text = re.split(stopping, text)[0]\n\n        return text\n</code></pre>"},{"location":"reference/processing/answer_processors/regex/#ragfit.processing.answer_processors.regex.RegexAnswer.__call__","title":"<code>__call__(text: str)</code>","text":"<p>Extract the answer from the text.</p> Source code in <code>ragfit/processing/answer_processors/regex.py</code> <pre><code>def __call__(self, text: str):\n    \"\"\"\n    Extract the answer from the text.\n    \"\"\"\n    if (capture := self.capture_pattern) and capture != \"\":\n        match = re.search(capture, text, re.MULTILINE | re.DOTALL)\n        if match:\n            text = match.group(1)\n\n    if (stopping := self.stopping_pattern) and stopping != \"\":\n        text = re.split(stopping, text)[0]\n\n    return text\n</code></pre>"},{"location":"reference/processing/dataset_loaders/loaders/","title":"loaders","text":""},{"location":"reference/processing/dataset_loaders/loaders/#ragfit.processing.dataset_loaders.loaders.HFLoader","title":"<code>HFLoader</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Class to load a dataset using the Hugging Face datasets library. Can use either a HuggingFace tag or a local file.</p> <p>Caching is disabled as this step does not manipulate the dataset hence no need for caching.</p> Source code in <code>ragfit/processing/dataset_loaders/loaders.py</code> <pre><code>class HFLoader(GlobalStep):\n    \"\"\"\n    Class to load a dataset using the Hugging Face datasets library.\n    Can use either a HuggingFace tag or a local file.\n\n    Caching is disabled as this step does not manipulate the dataset hence no need for caching.\n    \"\"\"\n\n    def __init__(self, dataset_config: dict, **kwargs):\n        super().__init__(**kwargs)\n        self.dataset_config = dataset_config\n        self.cache_step = False\n\n    def process(self, dataset_name, datasets, **kwargs):\n        datasets[dataset_name] = load_dataset(**self.dataset_config)\n</code></pre>"},{"location":"reference/processing/dataset_loaders/loaders/#ragfit.processing.dataset_loaders.loaders.LocalLoader","title":"<code>LocalLoader</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Class to load a local dataset, in a JSON format.</p> <p>Caching is disabled as this step does not manipulate the dataset hence no need for caching.</p> Source code in <code>ragfit/processing/dataset_loaders/loaders.py</code> <pre><code>class LocalLoader(GlobalStep):\n    \"\"\"\n    Class to load a local dataset, in a JSON format.\n\n    Caching is disabled as this step does not manipulate the dataset hence no need for caching.\n    \"\"\"\n\n    def __init__(self, filename, **kwargs):\n        super().__init__(**kwargs)\n        self.filename = filename\n        self.cache_step = False\n\n    def process(self, dataset_name, datasets, **kwargs):\n        datasets[dataset_name] = load_dataset(\n            \"json\",\n            data_files=self.filename,\n            split=\"train\",\n        )\n</code></pre>"},{"location":"reference/processing/global_steps/aggregation/","title":"Aggregation and merging","text":""},{"location":"reference/processing/global_steps/aggregation/#ragfit.processing.global_steps.aggregation.DatasetTagger","title":"<code>DatasetTagger</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Class to tag each example with the dataset name. Useful when running aggregations.</p> Source code in <code>ragfit/processing/global_steps/aggregation.py</code> <pre><code>class DatasetTagger(GlobalStep):\n    \"\"\"\n    Class to tag each example with the dataset name. Useful when running aggregations.\n    \"\"\"\n\n    def __init__(self, keyword=\"source\", **kwargs):\n        \"\"\"\n        Args:\n            keyword (str): The key to use for tagging. Default is \"source\".\n        \"\"\"\n        super().__init__(**kwargs)\n        self.keyword = keyword\n\n    def tag(self, item, dataset_name):\n        item[self.keyword] = dataset_name\n        return item\n\n    def process(self, dataset_name, datasets, **kwargs):\n        datasets[dataset_name] = datasets[dataset_name].map(\n            lambda item: self.tag(item, dataset_name),\n            load_from_cache_file=False,\n        )\n</code></pre>"},{"location":"reference/processing/global_steps/aggregation/#ragfit.processing.global_steps.aggregation.DatasetTagger.__init__","title":"<code>__init__(keyword='source', **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>keyword</code>               (<code>str</code>, default:                   <code>'source'</code> )           \u2013            <p>The key to use for tagging. Default is \"source\".</p> </li> </ul> Source code in <code>ragfit/processing/global_steps/aggregation.py</code> <pre><code>def __init__(self, keyword=\"source\", **kwargs):\n    \"\"\"\n    Args:\n        keyword (str): The key to use for tagging. Default is \"source\".\n    \"\"\"\n    super().__init__(**kwargs)\n    self.keyword = keyword\n</code></pre>"},{"location":"reference/processing/global_steps/aggregation/#ragfit.processing.global_steps.aggregation.FilterDataset","title":"<code>FilterDataset</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Step for filtering a dataset.</p> Source code in <code>ragfit/processing/global_steps/aggregation.py</code> <pre><code>class FilterDataset(GlobalStep):\n    \"\"\"\n    Step for filtering a dataset.\n    \"\"\"\n\n    def __init__(self, filter_fn, **kwargs):\n        \"\"\"\n        Args:\n            filter_fn (function): Function to filter the dataset.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.filter_fn = filters[filter_fn]\n\n    def process(self, dataset_name, datasets, **kwargs):\n        datasets[dataset_name] = datasets[dataset_name].filter(self.filter_fn)\n</code></pre>"},{"location":"reference/processing/global_steps/aggregation/#ragfit.processing.global_steps.aggregation.FilterDataset.__init__","title":"<code>__init__(filter_fn, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>filter_fn</code>               (<code>function</code>)           \u2013            <p>Function to filter the dataset.</p> </li> </ul> Source code in <code>ragfit/processing/global_steps/aggregation.py</code> <pre><code>def __init__(self, filter_fn, **kwargs):\n    \"\"\"\n    Args:\n        filter_fn (function): Function to filter the dataset.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.filter_fn = filters[filter_fn]\n</code></pre>"},{"location":"reference/processing/global_steps/aggregation/#ragfit.processing.global_steps.aggregation.MergeDatasets","title":"<code>MergeDatasets</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Step for merging datasets.</p> <p>Merge is done using concatenation. Optional shuffling by providing a seed.</p> Source code in <code>ragfit/processing/global_steps/aggregation.py</code> <pre><code>class MergeDatasets(GlobalStep):\n    \"\"\"\n    Step for merging datasets.\n\n    Merge is done using concatenation. Optional shuffling by providing a seed.\n    \"\"\"\n\n    def __init__(self, output, shuffle=None, **kwargs):\n        \"\"\"\n        Args:\n            output (str): Name of the output dataset. Should be unique.\n            shuffle (int, optional): seed for shuffling. Default is None.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.output = output\n        self.shuffle = shuffle\n        self.completed = False\n        self.cache_step = False\n\n    def process(self, dataset_name, datasets, **kwargs):\n        if not self.completed:\n            data = concatenate_datasets([datasets[name] for name in self.inputs])\n            if self.shuffle:\n                data = data.shuffle(self.shuffle)\n            datasets[self.output] = data\n            self.completed = True\n</code></pre>"},{"location":"reference/processing/global_steps/aggregation/#ragfit.processing.global_steps.aggregation.MergeDatasets.__init__","title":"<code>__init__(output, shuffle=None, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>output</code>               (<code>str</code>)           \u2013            <p>Name of the output dataset. Should be unique.</p> </li> <li> <code>shuffle</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>seed for shuffling. Default is None.</p> </li> </ul> Source code in <code>ragfit/processing/global_steps/aggregation.py</code> <pre><code>def __init__(self, output, shuffle=None, **kwargs):\n    \"\"\"\n    Args:\n        output (str): Name of the output dataset. Should be unique.\n        shuffle (int, optional): seed for shuffling. Default is None.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.output = output\n    self.shuffle = shuffle\n    self.completed = False\n    self.cache_step = False\n</code></pre>"},{"location":"reference/processing/global_steps/aggregation/#ragfit.processing.global_steps.aggregation.SelectColumns","title":"<code>SelectColumns</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Step for selecting specified columns in a dataset.</p> Source code in <code>ragfit/processing/global_steps/aggregation.py</code> <pre><code>class SelectColumns(GlobalStep):\n    \"\"\"\n    Step for selecting specified columns in a dataset.\n    \"\"\"\n\n    def __init__(self, columns: list[str], **kwargs):\n        \"\"\"\n        Args:\n            columns (list): List of keys to keep in the dataset.\n        \"\"\"\n        super().__init__(**kwargs)\n        assert isinstance(columns, list), \"columns should be a list of strings.\"\n        self.columns = columns\n\n    def process(self, dataset_name, datasets, **kwargs):\n        datasets[dataset_name] = datasets[dataset_name].select_columns(self.columns)\n</code></pre>"},{"location":"reference/processing/global_steps/aggregation/#ragfit.processing.global_steps.aggregation.SelectColumns.__init__","title":"<code>__init__(columns: list[str], **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>columns</code>               (<code>list</code>)           \u2013            <p>List of keys to keep in the dataset.</p> </li> </ul> Source code in <code>ragfit/processing/global_steps/aggregation.py</code> <pre><code>def __init__(self, columns: list[str], **kwargs):\n    \"\"\"\n    Args:\n        columns (list): List of keys to keep in the dataset.\n    \"\"\"\n    super().__init__(**kwargs)\n    assert isinstance(columns, list), \"columns should be a list of strings.\"\n    self.columns = columns\n</code></pre>"},{"location":"reference/processing/global_steps/filters/","title":"Filters","text":"<p>Module containing filters.</p>"},{"location":"reference/processing/global_steps/filters/#ragfit.processing.global_steps.filters.msmarco_positive_filter","title":"<code>msmarco_positive_filter(x)</code>","text":"<p>Identify the positive passages in MSMARCO dataset.</p> Source code in <code>ragfit/processing/global_steps/filters.py</code> <pre><code>def msmarco_positive_filter(x):\n    \"\"\"\n    Identify the positive passages in MSMARCO dataset.\n    \"\"\"\n    return 1 in x[\"passages\"][\"is_selected\"]\n</code></pre>"},{"location":"reference/processing/global_steps/output/","title":"Output","text":""},{"location":"reference/processing/global_steps/output/#ragfit.processing.global_steps.output.HFHubOutput","title":"<code>HFHubOutput</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Simple class to output the dataset to Hugging Face Hub.</p> <p>Caching is disabled as this step does not manipulate the dataset hence no need for caching.</p> Source code in <code>ragfit/processing/global_steps/output.py</code> <pre><code>class HFHubOutput(GlobalStep):\n    \"\"\"\n    Simple class to output the dataset to Hugging Face Hub.\n\n    Caching is disabled as this step does not manipulate the dataset hence no need for caching.\n    \"\"\"\n\n    def __init__(self, hfhub_tag, private=True, **kwargs):\n        \"\"\"\n        Args:\n            hfhub_tag (str): Tag for the Hugging Face Hub.\n            private (bool): Whether the dataset should be private or not. Default is True.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.hfhub_tag = hfhub_tag\n        self.private = private\n        self.cache_step = False\n\n    def process(self, dataset_name, datasets, **kwargs):\n        datasets[dataset_name].push_to_hub(self.hfhub_tag, private=self.private)\n</code></pre>"},{"location":"reference/processing/global_steps/output/#ragfit.processing.global_steps.output.HFHubOutput.__init__","title":"<code>__init__(hfhub_tag, private=True, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>hfhub_tag</code>               (<code>str</code>)           \u2013            <p>Tag for the Hugging Face Hub.</p> </li> <li> <code>private</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether the dataset should be private or not. Default is True.</p> </li> </ul> Source code in <code>ragfit/processing/global_steps/output.py</code> <pre><code>def __init__(self, hfhub_tag, private=True, **kwargs):\n    \"\"\"\n    Args:\n        hfhub_tag (str): Tag for the Hugging Face Hub.\n        private (bool): Whether the dataset should be private or not. Default is True.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.hfhub_tag = hfhub_tag\n    self.private = private\n    self.cache_step = False\n</code></pre>"},{"location":"reference/processing/global_steps/output/#ragfit.processing.global_steps.output.OutputData","title":"<code>OutputData</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Simple class to output the dataset to a jsonl file.</p> <p>Caching is disabled as this step does not manipulate the dataset hence no need for caching.</p> Source code in <code>ragfit/processing/global_steps/output.py</code> <pre><code>class OutputData(GlobalStep):\n    \"\"\"\n    Simple class to output the dataset to a jsonl file.\n\n    Caching is disabled as this step does not manipulate the dataset hence no need for caching.\n    \"\"\"\n\n    def __init__(self, prefix, filename=None, directory=None, **kwargs):\n        \"\"\"\n        Args:\n            prefix (str): Prefix for the output.\n            filename (str, optional): Name of the output file. If not provided, the output file name will be generated based on the prefix and dataset name.\n            directory (str, optional): Directory to save the output file. If not provided, the output file will be saved in the current directory.\n\n        The output name is `{prefix}-{dataset_keyname/filename}.jsonl` if `filename` is not provided.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.prefix = prefix\n        self.filename = filename\n        self.dir = directory\n        self.cache_step = False\n\n    def process(self, dataset_name, datasets, **kwargs):\n        if self.filename:\n            name = self.filename\n        else:\n            name = dataset_name\n        fname = f\"{self.prefix}-{name}.jsonl\"\n        if self.dir is not None:\n            fname = os.path.join(self.dir, fname) if self.dir else fname\n        datasets[dataset_name].to_json(fname, lines=True)\n</code></pre>"},{"location":"reference/processing/global_steps/output/#ragfit.processing.global_steps.output.OutputData.__init__","title":"<code>__init__(prefix, filename=None, directory=None, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>prefix</code>               (<code>str</code>)           \u2013            <p>Prefix for the output.</p> </li> <li> <code>filename</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Name of the output file. If not provided, the output file name will be generated based on the prefix and dataset name.</p> </li> <li> <code>directory</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Directory to save the output file. If not provided, the output file will be saved in the current directory.</p> </li> </ul> <p>The output name is <code>{prefix}-{dataset_keyname/filename}.jsonl</code> if <code>filename</code> is not provided.</p> Source code in <code>ragfit/processing/global_steps/output.py</code> <pre><code>def __init__(self, prefix, filename=None, directory=None, **kwargs):\n    \"\"\"\n    Args:\n        prefix (str): Prefix for the output.\n        filename (str, optional): Name of the output file. If not provided, the output file name will be generated based on the prefix and dataset name.\n        directory (str, optional): Directory to save the output file. If not provided, the output file will be saved in the current directory.\n\n    The output name is `{prefix}-{dataset_keyname/filename}.jsonl` if `filename` is not provided.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.prefix = prefix\n    self.filename = filename\n    self.dir = directory\n    self.cache_step = False\n</code></pre>"},{"location":"reference/processing/global_steps/sampling/","title":"Sampling and Fewshot","text":""},{"location":"reference/processing/global_steps/sampling/#ragfit.processing.global_steps.sampling.FewShot","title":"<code>FewShot</code>","text":"<p>               Bases: <code>Sampler</code></p> <p>Class to collect fewshot examples from the same or another dataset.</p> Source code in <code>ragfit/processing/global_steps/sampling.py</code> <pre><code>class FewShot(Sampler):\n    \"\"\"\n    Class to collect fewshot examples from the same or another dataset.\n    \"\"\"\n\n    def __init__(self, k, output_key=\"fewshot\", input_dataset=None, **kwargs):\n        \"\"\"\n        Args:\n            k (int): Number of examples to collect.\n            output_key (str): output key to use for the collected examples.\n            input_dataset (str): Name of the dataset to take the examples from. To use the same dataset, use None.\n        \"\"\"\n        super().__init__(\n            k=k,\n            output_key=output_key,\n            input_key=None,\n            input_dataset=input_dataset,\n            **kwargs,\n        )\n</code></pre>"},{"location":"reference/processing/global_steps/sampling/#ragfit.processing.global_steps.sampling.FewShot.__init__","title":"<code>__init__(k, output_key='fewshot', input_dataset=None, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>Number of examples to collect.</p> </li> <li> <code>output_key</code>               (<code>str</code>, default:                   <code>'fewshot'</code> )           \u2013            <p>output key to use for the collected examples.</p> </li> <li> <code>input_dataset</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Name of the dataset to take the examples from. To use the same dataset, use None.</p> </li> </ul> Source code in <code>ragfit/processing/global_steps/sampling.py</code> <pre><code>def __init__(self, k, output_key=\"fewshot\", input_dataset=None, **kwargs):\n    \"\"\"\n    Args:\n        k (int): Number of examples to collect.\n        output_key (str): output key to use for the collected examples.\n        input_dataset (str): Name of the dataset to take the examples from. To use the same dataset, use None.\n    \"\"\"\n    super().__init__(\n        k=k,\n        output_key=output_key,\n        input_key=None,\n        input_dataset=input_dataset,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/processing/global_steps/sampling/#ragfit.processing.global_steps.sampling.Sampler","title":"<code>Sampler</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Class to augment a dataset with sampled examples from the same or another dataset.</p> <p>Full examples can be collected, as well as an individual example keys like <code>query</code>, <code>documents</code>, etc.</p> <p>The step can be used to collect negative documents, negative queries and collect fewshot examples. For fewshot examples, use the dedicated <code>FewShot</code> class.</p> Source code in <code>ragfit/processing/global_steps/sampling.py</code> <pre><code>class Sampler(GlobalStep):\n    \"\"\"\n    Class to augment a dataset with sampled examples from the same or another dataset.\n\n    Full examples can be collected, as well as an individual example keys like `query`, `documents`, etc.\n\n    The step can be used to collect negative documents, negative queries and collect fewshot examples.\n    For fewshot examples, use the dedicated `FewShot` class.\n    \"\"\"\n\n    def __init__(\n        self, k, input_key=None, output_key=\"fewshot\", input_dataset=None, **kwargs\n    ):\n        \"\"\"\n        Args:\n            k (int): Number of examples to collect.\n            input_key (str): a key to collect from the collected examples, or None to take entire example.\n            output_key (str): output key to use for the examples.\n            input_dataset (str): Name of the dataset to take the examples from. To use the same dataset, use None.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.k = k\n        self.input_key = input_key\n        self.input_dataset = input_dataset\n        self.output_key = output_key\n\n    def process(self, dataset_name, datasets, **kwargs):\n        input_dataset = datasets[self.input_dataset or dataset_name]\n\n        def find_examples(item, idx):\n            ids = []\n            while len(ids) &lt; self.k:\n                rand_idx = random.randint(0, len(input_dataset) - 1)\n                if self.input_dataset is None and rand_idx == idx:\n                    continue\n                if rand_idx in ids:\n                    continue\n                ids.append(rand_idx)\n            examples = [\n                (\n                    input_dataset[id_]\n                    if self.input_key is None\n                    else input_dataset[id_][self.input_key]\n                )\n                for id_ in ids\n            ]\n            item[self.output_key] = examples if self.k &gt; 1 else examples[0]\n            return item\n\n        datasets[dataset_name] = datasets[dataset_name].map(\n            lambda item, index: find_examples(item, index),\n            with_indices=True,\n            load_from_cache_file=False,\n        )\n</code></pre>"},{"location":"reference/processing/global_steps/sampling/#ragfit.processing.global_steps.sampling.Sampler.__init__","title":"<code>__init__(k, input_key=None, output_key='fewshot', input_dataset=None, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>Number of examples to collect.</p> </li> <li> <code>input_key</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>a key to collect from the collected examples, or None to take entire example.</p> </li> <li> <code>output_key</code>               (<code>str</code>, default:                   <code>'fewshot'</code> )           \u2013            <p>output key to use for the examples.</p> </li> <li> <code>input_dataset</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Name of the dataset to take the examples from. To use the same dataset, use None.</p> </li> </ul> Source code in <code>ragfit/processing/global_steps/sampling.py</code> <pre><code>def __init__(\n    self, k, input_key=None, output_key=\"fewshot\", input_dataset=None, **kwargs\n):\n    \"\"\"\n    Args:\n        k (int): Number of examples to collect.\n        input_key (str): a key to collect from the collected examples, or None to take entire example.\n        output_key (str): output key to use for the examples.\n        input_dataset (str): Name of the dataset to take the examples from. To use the same dataset, use None.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.k = k\n    self.input_key = input_key\n    self.input_dataset = input_dataset\n    self.output_key = output_key\n</code></pre>"},{"location":"reference/processing/global_steps/sampling/#ragfit.processing.global_steps.sampling.ShuffleSelect","title":"<code>ShuffleSelect</code>","text":"<p>               Bases: <code>GlobalStep</code></p> <p>Class to optionally shuffle and select a subset of the dataset.</p> <p>Based on the <code>shuffle</code> and <code>select</code> methods of HF Dataset.</p> Source code in <code>ragfit/processing/global_steps/sampling.py</code> <pre><code>class ShuffleSelect(GlobalStep):\n    \"\"\"\n    Class to optionally shuffle and select a subset of the dataset.\n\n    Based on the `shuffle` and `select` methods of HF Dataset.\n    \"\"\"\n\n    def __init__(self, shuffle=None, limit=None, **kwargs):\n        \"\"\"\n        Args:\n            shuffle (int): Seed for shuffling the dataset.\n            limit (int): Number of items to select from the dataset.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.shuffle = shuffle\n        self.limit = limit\n\n    def process_all(self, dataset, datasets, **kwargs):\n        if self.shuffle:\n            dataset = dataset.shuffle(seed=self.shuffle)\n        if self.limit:\n            dataset = dataset.select(range(min(len(dataset), self.limit)))\n        return dataset\n</code></pre>"},{"location":"reference/processing/global_steps/sampling/#ragfit.processing.global_steps.sampling.ShuffleSelect.__init__","title":"<code>__init__(shuffle=None, limit=None, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>shuffle</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Seed for shuffling the dataset.</p> </li> <li> <code>limit</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of items to select from the dataset.</p> </li> </ul> Source code in <code>ragfit/processing/global_steps/sampling.py</code> <pre><code>def __init__(self, shuffle=None, limit=None, **kwargs):\n    \"\"\"\n    Args:\n        shuffle (int): Seed for shuffling the dataset.\n        limit (int): Number of items to select from the dataset.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.shuffle = shuffle\n    self.limit = limit\n</code></pre>"},{"location":"reference/processing/local_steps/common_datasets/","title":"Common Datasets","text":""},{"location":"reference/processing/local_steps/common_datasets/#ragfit.processing.local_steps.common_datasets.ARCC","title":"<code>ARCC</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Prepare dataset for RAG augmentation.</p> Source code in <code>ragfit/processing/local_steps/common_datasets.py</code> <pre><code>class ARCC(LocalStep):\n    \"\"\"\n    Prepare dataset for RAG augmentation.\n    \"\"\"\n\n    def process_item(self, item, index, datasets, **kwargs):\n        item[\"query\"] = item[\"question\"]\n        item[\"options\"] = item[\"choices\"][\"text\"]\n        item[\"answers\"] = item[\"answerKey\"]\n\n        return item\n</code></pre>"},{"location":"reference/processing/local_steps/common_datasets/#ragfit.processing.local_steps.common_datasets.ASQA","title":"<code>ASQA</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Normalizes ASQA dataset.</p> <p>It has long answer, to be measured with ROUGE-L and multiple short answers, to be measured with string-EM. Long answer is saved in the <code>answers</code> field, while the short answers (list of lists) are saved in the <code>answer-short</code> field.</p> Source code in <code>ragfit/processing/local_steps/common_datasets.py</code> <pre><code>class ASQA(LocalStep):\n    \"\"\"\n    Normalizes ASQA dataset.\n\n    It has long answer, to be measured with ROUGE-L and multiple short answers, to be\n    measured with string-EM. Long answer is saved in the `answers` field, while the\n    short answers (list of lists) are saved in the `answer-short` field.\n    \"\"\"\n\n    def process_item(self, item, index, datasets, **kwargs):\n        item[\"answer-long\"] = [ann[\"long_answer\"] for ann in item[\"annotations\"]]\n        short = []\n        for qa_pair in item[\"qa_pairs\"]:\n            normalize = [normalize_text(ans) for ans in qa_pair[\"short_answers\"]]\n            short.append(normalize)\n        item[\"answer-short\"] = short\n\n        item[\"answers\"] = item[\"answer-long\"]\n        item[\"query\"] = item[\"ambiguous_question\"]\n\n        return item\n</code></pre>"},{"location":"reference/processing/local_steps/common_datasets/#ragfit.processing.local_steps.common_datasets.HotPot","title":"<code>HotPot</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Normalizes NotPotQA dataset to look like NQ, TQA</p> Source code in <code>ragfit/processing/local_steps/common_datasets.py</code> <pre><code>class HotPot(LocalStep):\n    \"\"\"\n    Normalizes NotPotQA dataset to look like NQ, TQA\n    \"\"\"\n\n    def process_item(self, item, index, datasets, **kwargs):\n        item[\"answers\"] = [item[\"answer\"]]\n        item[\"query\"] = item[\"question\"]\n\n        # Contexts converted into a list of relevant documents (Dict with title + text)\n        titles = item[\"context\"][\"title\"]\n\n        sentences = item[\"context\"][\"sentences\"]\n        sentences = [\"\".join(lines) for lines in sentences]\n\n        docs = [f\"{title}: {text}\" for title, text in zip(titles, sentences)]\n        item[\"positive_passages\"] = docs\n\n        return item\n</code></pre>"},{"location":"reference/processing/local_steps/common_datasets/#ragfit.processing.local_steps.common_datasets.PubMed","title":"<code>PubMed</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Prepare dataset for RAG augmentation.</p> Source code in <code>ragfit/processing/local_steps/common_datasets.py</code> <pre><code>class PubMed(LocalStep):\n    \"\"\"\n    Prepare dataset for RAG augmentation.\n    \"\"\"\n\n    def process_item(self, item, index, datasets, **kwargs):\n        item[\"query\"] = item[\"QUESTION\"]\n        item[\"answers\"] = [item[\"final_decision\"]]\n        docs = item[\"CONTEXTS\"]\n        item[\"positive_passages\"] = docs\n\n        return item\n</code></pre>"},{"location":"reference/processing/local_steps/context/","title":"Context","text":""},{"location":"reference/processing/local_steps/context/#ragfit.processing.local_steps.context.ContextHandler","title":"<code>ContextHandler</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Example class for processing retrieved documents.</p> <p>In this simple example, the text is combined with the title.</p> Source code in <code>ragfit/processing/local_steps/context.py</code> <pre><code>class ContextHandler(LocalStep):\n    \"\"\"\n    Example class for processing retrieved documents.\n\n    In this simple example, the text is combined with the title.\n    \"\"\"\n\n    def __init__(self, docs_key, title_key=\"title\", text_key=\"content\", **kwargs):\n        \"\"\"\n        Args:\n            docs_key (str): Key to the documents in the item.\n            title_key (str): Key to the title in the document.\n            text_key (str): Key to the text in the document.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.docs_key = docs_key\n        self.title_key = title_key\n        self.text_key = text_key\n\n    def process_item(self, item, index, datasets, **kwargs):\n        docs = item[self.docs_key]\n        docs = [f\"{doc[self.title_key]}: {doc[self.text_key]}\" for doc in docs]\n        item[self.docs_key] = docs\n        return item\n</code></pre>"},{"location":"reference/processing/local_steps/context/#ragfit.processing.local_steps.context.ContextHandler.__init__","title":"<code>__init__(docs_key, title_key='title', text_key='content', **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>docs_key</code>               (<code>str</code>)           \u2013            <p>Key to the documents in the item.</p> </li> <li> <code>title_key</code>               (<code>str</code>, default:                   <code>'title'</code> )           \u2013            <p>Key to the title in the document.</p> </li> <li> <code>text_key</code>               (<code>str</code>, default:                   <code>'content'</code> )           \u2013            <p>Key to the text in the document.</p> </li> </ul> Source code in <code>ragfit/processing/local_steps/context.py</code> <pre><code>def __init__(self, docs_key, title_key=\"title\", text_key=\"content\", **kwargs):\n    \"\"\"\n    Args:\n        docs_key (str): Key to the documents in the item.\n        title_key (str): Key to the title in the document.\n        text_key (str): Key to the text in the document.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.docs_key = docs_key\n    self.title_key = title_key\n    self.text_key = text_key\n</code></pre>"},{"location":"reference/processing/local_steps/context/#ragfit.processing.local_steps.context.DocumentsJoiner","title":"<code>DocumentsJoiner</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Class to select top-K and join the documents into a string.</p> Source code in <code>ragfit/processing/local_steps/context.py</code> <pre><code>class DocumentsJoiner(LocalStep):\n    \"\"\"\n    Class to select top-K and join the documents into a string.\n    \"\"\"\n\n    def __init__(self, docs_key, k=None, join_string=\"\\n\", **kwargs):\n        \"\"\"\n        Join `k` documents in `docs_key` into a string using `join_string` and save back to `docs_key`.\n\n        Args:\n            docs_key (str): Key to the documents in the item.\n            k (int, optional): Number of documents to select or take all. Defaults to None.\n            join_string (str): String to join the documents. Defaults to \"\\\\n\".\n        \"\"\"\n        super().__init__(**kwargs)\n        self.docs_key = docs_key\n        self.k = k\n        self.join_string = join_string\n\n    def process_item(self, item, index, datasets, **kwargs):\n        docs = item[self.docs_key]\n        if self.k is not None:\n            docs = docs[: self.k]\n        docs = self.join_string.join(docs)\n        item[self.docs_key] = docs\n        return item\n</code></pre>"},{"location":"reference/processing/local_steps/context/#ragfit.processing.local_steps.context.DocumentsJoiner.__init__","title":"<code>__init__(docs_key, k=None, join_string='\\n', **kwargs)</code>","text":"<p>Join <code>k</code> documents in <code>docs_key</code> into a string using <code>join_string</code> and save back to <code>docs_key</code>.</p> <p>Parameters:</p> <ul> <li> <code>docs_key</code>               (<code>str</code>)           \u2013            <p>Key to the documents in the item.</p> </li> <li> <code>k</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Number of documents to select or take all. Defaults to None.</p> </li> <li> <code>join_string</code>               (<code>str</code>, default:                   <code>'\\n'</code> )           \u2013            <p>String to join the documents. Defaults to \"\\n\".</p> </li> </ul> Source code in <code>ragfit/processing/local_steps/context.py</code> <pre><code>def __init__(self, docs_key, k=None, join_string=\"\\n\", **kwargs):\n    \"\"\"\n    Join `k` documents in `docs_key` into a string using `join_string` and save back to `docs_key`.\n\n    Args:\n        docs_key (str): Key to the documents in the item.\n        k (int, optional): Number of documents to select or take all. Defaults to None.\n        join_string (str): String to join the documents. Defaults to \"\\\\n\".\n    \"\"\"\n    super().__init__(**kwargs)\n    self.docs_key = docs_key\n    self.k = k\n    self.join_string = join_string\n</code></pre>"},{"location":"reference/processing/local_steps/formatting/","title":"Formatting","text":""},{"location":"reference/processing/local_steps/formatting/#ragfit.processing.local_steps.formatting.ColumnUpdater","title":"<code>ColumnUpdater</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Simple class to create new columns from existing columns in a dataset. Existing columns are not modified.</p> <p>Parameters:</p> <ul> <li> <code>keys_mapping</code>               (<code>dict</code>)           \u2013            <p>Dictionary with \"from:to\" mapping.</p> </li> </ul> Source code in <code>ragfit/processing/local_steps/formatting.py</code> <pre><code>class ColumnUpdater(LocalStep):\n    \"\"\"\n    Simple class to create new columns from existing columns in a dataset.\n    Existing columns are not modified.\n\n    Args:\n        keys_mapping (dict): Dictionary with \"from:to\" mapping.\n    \"\"\"\n\n    def __init__(self, keys_mapping: dict, **kwargs):\n        super().__init__(**kwargs)\n        self.keys_mapping = keys_mapping\n\n    def process_item(self, item, index, datasets, **kwargs):\n        for from_key, to_key in self.keys_mapping.items():\n            item[to_key] = item[from_key]\n        return item\n</code></pre>"},{"location":"reference/processing/local_steps/formatting/#ragfit.processing.local_steps.formatting.FlattenList","title":"<code>FlattenList</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Class to join a list of strings into a single string.</p> Source code in <code>ragfit/processing/local_steps/formatting.py</code> <pre><code>class FlattenList(LocalStep):\n    \"\"\"\n    Class to join a list of strings into a single string.\n    \"\"\"\n\n    def __init__(self, input_key, output_key, string_join=\", \", **kwargs):\n        \"\"\"\n        Args:\n            input_key (str): Key to the list of strings.\n            output_key (str): Key to store the joined string.\n            string_join (str): String to join the list of strings. Defaults to \", \".\n        \"\"\"\n        super().__init__(**kwargs)\n        self.input_key = input_key\n        self.output_key = output_key\n        self.string_join = string_join\n\n    def process_item(self, item, index, datasets, **kwargs):\n        item[self.output_key] = self.string_join.join(item[self.input_key])\n        return item\n</code></pre>"},{"location":"reference/processing/local_steps/formatting/#ragfit.processing.local_steps.formatting.FlattenList.__init__","title":"<code>__init__(input_key, output_key, string_join=', ', **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>input_key</code>               (<code>str</code>)           \u2013            <p>Key to the list of strings.</p> </li> <li> <code>output_key</code>               (<code>str</code>)           \u2013            <p>Key to store the joined string.</p> </li> <li> <code>string_join</code>               (<code>str</code>, default:                   <code>', '</code> )           \u2013            <p>String to join the list of strings. Defaults to \", \".</p> </li> </ul> Source code in <code>ragfit/processing/local_steps/formatting.py</code> <pre><code>def __init__(self, input_key, output_key, string_join=\", \", **kwargs):\n    \"\"\"\n    Args:\n        input_key (str): Key to the list of strings.\n        output_key (str): Key to store the joined string.\n        string_join (str): String to join the list of strings. Defaults to \", \".\n    \"\"\"\n    super().__init__(**kwargs)\n    self.input_key = input_key\n    self.output_key = output_key\n    self.string_join = string_join\n</code></pre>"},{"location":"reference/processing/local_steps/formatting/#ragfit.processing.local_steps.formatting.UpdateField","title":"<code>UpdateField</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Class to update a field in the dataset with a new value.</p> Source code in <code>ragfit/processing/local_steps/formatting.py</code> <pre><code>class UpdateField(LocalStep):\n    \"\"\"\n    Class to update a field in the dataset with a new value.\n    \"\"\"\n\n    def __init__(self, input_key: str, value, **kwargs):\n        \"\"\"\n        Args:\n            input_key (str): example key to change.\n            value: New value to set for the field.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.input_key = input_key\n        self.value = value\n\n    def process_item(self, item, index, datasets, **kwargs):\n        item[self.input_key] = self.value\n        return item\n</code></pre>"},{"location":"reference/processing/local_steps/formatting/#ragfit.processing.local_steps.formatting.UpdateField.__init__","title":"<code>__init__(input_key: str, value, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>input_key</code>               (<code>str</code>)           \u2013            <p>example key to change.</p> </li> <li> <code>value</code>           \u2013            <p>New value to set for the field.</p> </li> </ul> Source code in <code>ragfit/processing/local_steps/formatting.py</code> <pre><code>def __init__(self, input_key: str, value, **kwargs):\n    \"\"\"\n    Args:\n        input_key (str): example key to change.\n        value: New value to set for the field.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.input_key = input_key\n    self.value = value\n</code></pre>"},{"location":"reference/processing/local_steps/inference/","title":"Inference","text":"<p>Module for inference steps, which can use LLM output to augment the data.</p>"},{"location":"reference/processing/local_steps/inference/#ragfit.processing.local_steps.inference.HFStep","title":"<code>HFStep</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Class for running inference with a Hugging Face model based on the vLLM engine.</p> Source code in <code>ragfit/processing/local_steps/inference.py</code> <pre><code>class HFStep(LocalStep):\n    \"\"\"\n    Class for running inference with a Hugging Face model based on the vLLM engine.\n    \"\"\"\n\n    def __init__(self, input_key, output_key, model_kwargs, **kwargs):\n        \"\"\"\n        Initialize the HFStep class.\n\n        Args:\n                input_key (str): The key for the input text to be served as the prompt.\n                output_key (str): The key for for saving the generated text.\n                model_kwargs (dict): The keyword arguments to pass to the vLLM model.\n                **kwargs: Additional keyword arguments to pass to the LocalStep.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.input_key = input_key\n        self.output_key = output_key\n        self.model_kwargs = model_kwargs\n        self.model = VLLMInference(**model_kwargs)\n\n    def process_item(self, item, index, datasets, **kwargs):\n        prompt = item[self.input_key]\n        response = self.model.generate(prompt)\n        item[self.output_key] = response\n        return item\n</code></pre>"},{"location":"reference/processing/local_steps/inference/#ragfit.processing.local_steps.inference.HFStep.__init__","title":"<code>__init__(input_key, output_key, model_kwargs, **kwargs)</code>","text":"<p>Initialize the HFStep class.</p> <p>Parameters:</p> <ul> <li> <code>input_key</code>               (<code>str</code>)           \u2013            <p>The key for the input text to be served as the prompt.</p> </li> <li> <code>output_key</code>               (<code>str</code>)           \u2013            <p>The key for for saving the generated text.</p> </li> <li> <code>model_kwargs</code>               (<code>dict</code>)           \u2013            <p>The keyword arguments to pass to the vLLM model.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments to pass to the LocalStep.</p> </li> </ul> Source code in <code>ragfit/processing/local_steps/inference.py</code> <pre><code>def __init__(self, input_key, output_key, model_kwargs, **kwargs):\n    \"\"\"\n    Initialize the HFStep class.\n\n    Args:\n            input_key (str): The key for the input text to be served as the prompt.\n            output_key (str): The key for for saving the generated text.\n            model_kwargs (dict): The keyword arguments to pass to the vLLM model.\n            **kwargs: Additional keyword arguments to pass to the LocalStep.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.input_key = input_key\n    self.output_key = output_key\n    self.model_kwargs = model_kwargs\n    self.model = VLLMInference(**model_kwargs)\n</code></pre>"},{"location":"reference/processing/local_steps/prompter/","title":"Prompt Creation","text":""},{"location":"reference/processing/local_steps/prompter/#ragfit.processing.local_steps.prompter.FewshotPrompter","title":"<code>FewshotPrompter</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Class for formatting fewshot examples into a string, to be used in a prompt.</p> <p>The prompt template contains a placeholder for the fewshot examples; this class is used to format the examples into a string.</p> Source code in <code>ragfit/processing/local_steps/prompter.py</code> <pre><code>class FewshotPrompter(LocalStep):\n    \"\"\"\n    Class for formatting fewshot examples into a string, to be used in a prompt.\n\n    The prompt template contains a placeholder for the fewshot examples; this\n    class is used to format the examples into a string.\n    \"\"\"\n\n    def __init__(\n        self, prompt_file: str, fewshot_key: str, mapping: dict, output_key: str, **kwargs\n    ):\n        \"\"\"\n        Args:\n            prompt_file (str): Path to the prompt file for the individual fewshot examples.\n            fewshot_key (str): Key to the fewshot examples in the item.\n            mapping (dict): Mapping of the placeholders in the prompt to the item keys.\n            output_key (str): Key to store the formatted fewshot examples.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.prompt = open(prompt_file).read()\n        self.fewshot_key = fewshot_key\n        self.mapping = mapping\n        self.output_key = output_key\n\n    def process_item(self, item, index, datasets, **kwargs):\n        texts = []\n        for ex in item[self.fewshot_key]:\n            text = self.prompt.format(\n                **{k: ex.get(v, \"\") for k, v in self.mapping.items()}\n            )\n            texts.append(text)\n\n        item[self.output_key] = \"\\n\\n\".join(texts)\n        return item\n</code></pre>"},{"location":"reference/processing/local_steps/prompter/#ragfit.processing.local_steps.prompter.FewshotPrompter.__init__","title":"<code>__init__(prompt_file: str, fewshot_key: str, mapping: dict, output_key: str, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>prompt_file</code>               (<code>str</code>)           \u2013            <p>Path to the prompt file for the individual fewshot examples.</p> </li> <li> <code>fewshot_key</code>               (<code>str</code>)           \u2013            <p>Key to the fewshot examples in the item.</p> </li> <li> <code>mapping</code>               (<code>dict</code>)           \u2013            <p>Mapping of the placeholders in the prompt to the item keys.</p> </li> <li> <code>output_key</code>               (<code>str</code>)           \u2013            <p>Key to store the formatted fewshot examples.</p> </li> </ul> Source code in <code>ragfit/processing/local_steps/prompter.py</code> <pre><code>def __init__(\n    self, prompt_file: str, fewshot_key: str, mapping: dict, output_key: str, **kwargs\n):\n    \"\"\"\n    Args:\n        prompt_file (str): Path to the prompt file for the individual fewshot examples.\n        fewshot_key (str): Key to the fewshot examples in the item.\n        mapping (dict): Mapping of the placeholders in the prompt to the item keys.\n        output_key (str): Key to store the formatted fewshot examples.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.prompt = open(prompt_file).read()\n    self.fewshot_key = fewshot_key\n    self.mapping = mapping\n    self.output_key = output_key\n</code></pre>"},{"location":"reference/processing/local_steps/prompter/#ragfit.processing.local_steps.prompter.TextPrompter","title":"<code>TextPrompter</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Class for creating prompts. The input is a prompt file with placeholders, a mapping of the placeholders to the item keys, and the key to store the result.</p> Source code in <code>ragfit/processing/local_steps/prompter.py</code> <pre><code>class TextPrompter(LocalStep):\n    \"\"\"\n    Class for creating prompts. The input is a prompt file with placeholders, a mapping of the placeholders to the item keys, and the key to store the result.\n    \"\"\"\n\n    def __init__(self, prompt_file: str, mapping: dict, output_key, **kwargs):\n        \"\"\"\n        Args:\n            prompt_file (str): Path to the prompt file.\n            mapping (dict): Mapping of the placeholders in the prompt to the item keys.\n            output_key (str): Key to store the formatted prompt.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.prompt = open(prompt_file).read()\n        self.mapping = mapping\n        self.output_key = output_key\n\n    def process_item(self, item, index, datasets, **kwargs):\n        prompt = self.prompt.format(\n            **{k: item.get(v, \"\") for k, v in self.mapping.items()}\n        )\n        item[self.output_key] = prompt\n        return item\n</code></pre>"},{"location":"reference/processing/local_steps/prompter/#ragfit.processing.local_steps.prompter.TextPrompter.__init__","title":"<code>__init__(prompt_file: str, mapping: dict, output_key, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>prompt_file</code>               (<code>str</code>)           \u2013            <p>Path to the prompt file.</p> </li> <li> <code>mapping</code>               (<code>dict</code>)           \u2013            <p>Mapping of the placeholders in the prompt to the item keys.</p> </li> <li> <code>output_key</code>               (<code>str</code>)           \u2013            <p>Key to store the formatted prompt.</p> </li> </ul> Source code in <code>ragfit/processing/local_steps/prompter.py</code> <pre><code>def __init__(self, prompt_file: str, mapping: dict, output_key, **kwargs):\n    \"\"\"\n    Args:\n        prompt_file (str): Path to the prompt file.\n        mapping (dict): Mapping of the placeholders in the prompt to the item keys.\n        output_key (str): Key to store the formatted prompt.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.prompt = open(prompt_file).read()\n    self.mapping = mapping\n    self.output_key = output_key\n</code></pre>"},{"location":"reference/processing/local_steps/raft/","title":"RAFT","text":""},{"location":"reference/processing/local_steps/raft/#ragfit.processing.local_steps.raft.RAFTStep","title":"<code>RAFTStep</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Implementation of RAFT: Adapting Language Model to Domain Specific RAG.</p> <p>This class compiles a list of negative documents with probability <code>raft_p</code>, and a combination of positive and negative documents with probability 1 - <code>raft_p</code>.</p> <p>Zhang, Tianjun, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E. Gonzalez. 2024. \u201cRAFT: Adapting Language Model to Domain Specific RAG.\u201d arXiv. http://arxiv.org/abs/2403.10131.</p> Source code in <code>ragfit/processing/local_steps/raft.py</code> <pre><code>class RAFTStep(LocalStep):\n    \"\"\"\n    Implementation of RAFT: Adapting Language Model to Domain Specific RAG.\n\n    This class compiles a list of negative documents with probability `raft_p`,\n    and a combination of positive and negative documents with probability 1 - `raft_p`.\n\n    Zhang, Tianjun, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia,\n    Ion Stoica, and Joseph E. Gonzalez. 2024. \u201cRAFT: Adapting Language Model\n    to Domain Specific RAG.\u201d arXiv. http://arxiv.org/abs/2403.10131.\n    \"\"\"\n\n    def __init__(\n        self,\n        k: int = 5,\n        raft_p=0.5,\n        neg_docs_num=5,\n        positive_key=\"positive_passages\",\n        negative_key=\"negative_passages\",\n        output_key=\"docs\",\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            k (int): The number of positive passages to consider.\n            raft_p (float, optional): The probability of using positive passages. Defaults to 0.5.\n            neg_docs_num (int, optional): The number of negative passages to consider. Defaults to 2.\n            positive_key (str, optional): The key containing the positive passages. Defaults to \"positive_passages\".\n            negative_key (str, optional): The key containing the negative passages. Defaults to \"negative_passages\".\n            output_key (str, optional): The key to store the output. Defaults to \"docs\".\n        \"\"\"\n        super().__init__(**kwargs)\n        self.k = k\n        self.raft_p = raft_p\n        self.neg_docs_num = neg_docs_num\n        self.positive_key = positive_key\n        self.negative_key = negative_key\n        self.output_key = output_key\n\n    def process_item(self, item: dict, index, datasets, **kwargs):\n        docs_pos = item[self.positive_key]\n        docs_neg = item.get(self.negative_key, [])\n\n        p = random.random()  # nosec\n        oracle = 0\n\n        if p &gt; self.raft_p:\n            docs = docs_pos[: self.k] + docs_neg[: self.neg_docs_num]\n        else:\n            docs = docs_neg[: self.neg_docs_num]\n\n        item[self.output_key] = docs\n\n        return item\n</code></pre>"},{"location":"reference/processing/local_steps/raft/#ragfit.processing.local_steps.raft.RAFTStep.__init__","title":"<code>__init__(k: int = 5, raft_p=0.5, neg_docs_num=5, positive_key='positive_passages', negative_key='negative_passages', output_key='docs', **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>k</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>The number of positive passages to consider.</p> </li> <li> <code>raft_p</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The probability of using positive passages. Defaults to 0.5.</p> </li> <li> <code>neg_docs_num</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>The number of negative passages to consider. Defaults to 2.</p> </li> <li> <code>positive_key</code>               (<code>str</code>, default:                   <code>'positive_passages'</code> )           \u2013            <p>The key containing the positive passages. Defaults to \"positive_passages\".</p> </li> <li> <code>negative_key</code>               (<code>str</code>, default:                   <code>'negative_passages'</code> )           \u2013            <p>The key containing the negative passages. Defaults to \"negative_passages\".</p> </li> <li> <code>output_key</code>               (<code>str</code>, default:                   <code>'docs'</code> )           \u2013            <p>The key to store the output. Defaults to \"docs\".</p> </li> </ul> Source code in <code>ragfit/processing/local_steps/raft.py</code> <pre><code>def __init__(\n    self,\n    k: int = 5,\n    raft_p=0.5,\n    neg_docs_num=5,\n    positive_key=\"positive_passages\",\n    negative_key=\"negative_passages\",\n    output_key=\"docs\",\n    **kwargs,\n):\n    \"\"\"\n    Args:\n        k (int): The number of positive passages to consider.\n        raft_p (float, optional): The probability of using positive passages. Defaults to 0.5.\n        neg_docs_num (int, optional): The number of negative passages to consider. Defaults to 2.\n        positive_key (str, optional): The key containing the positive passages. Defaults to \"positive_passages\".\n        negative_key (str, optional): The key containing the negative passages. Defaults to \"negative_passages\".\n        output_key (str, optional): The key to store the output. Defaults to \"docs\".\n    \"\"\"\n    super().__init__(**kwargs)\n    self.k = k\n    self.raft_p = raft_p\n    self.neg_docs_num = neg_docs_num\n    self.positive_key = positive_key\n    self.negative_key = negative_key\n    self.output_key = output_key\n</code></pre>"},{"location":"reference/processing/local_steps/api/openai/","title":"OpenAI Chat","text":""},{"location":"reference/processing/local_steps/api/openai/#ragfit.processing.local_steps.api.openai.OpenAIChat","title":"<code>OpenAIChat</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Interaction with OpenAI service.</p> <p>Model is represented by the <code>OpenAIExecutor</code>.</p> <p>This step is a wrapper, extracting the prompt from the item, interact with the API, and saves the response to the <code>answer</code> key in the item.</p> Source code in <code>ragfit/processing/local_steps/api/openai.py</code> <pre><code>class OpenAIChat(LocalStep):\n    \"\"\"\n    Interaction with OpenAI service.\n\n    Model is represented by the `OpenAIExecutor`.\n\n    This step is a wrapper, extracting the prompt from the item, interact with the API, and\n    saves the response to the `answer` key in the item.\n    \"\"\"\n\n    def __init__(self, model, instruction, prompt_key, answer_key, **kwargs):\n        \"\"\"\n        Args:\n            model (dict): Configuration for the OpenAIExecutor.\n            instruction (str): Path to the system instruction file.\n            prompt_key (str): Key to the prompt in the item.\n            answer_key (str): Key to store the response.\n        \"\"\"\n        super().__init__(**kwargs)\n\n        self.model = OpenAIExecutor(**model)\n        self.prompt_key = prompt_key\n        self.answer_key = answer_key\n        self.instruction = open(instruction).read()\n\n    def process_item(self, item, index, datasets, **kwargs):\n        answer = self.model.chat(item[self.prompt_key], self.instruction)\n        item[self.answer_key] = answer\n        return item\n</code></pre>"},{"location":"reference/processing/local_steps/api/openai/#ragfit.processing.local_steps.api.openai.OpenAIChat.__init__","title":"<code>__init__(model, instruction, prompt_key, answer_key, **kwargs)</code>","text":"<p>Parameters:</p> <ul> <li> <code>model</code>               (<code>dict</code>)           \u2013            <p>Configuration for the OpenAIExecutor.</p> </li> <li> <code>instruction</code>               (<code>str</code>)           \u2013            <p>Path to the system instruction file.</p> </li> <li> <code>prompt_key</code>               (<code>str</code>)           \u2013            <p>Key to the prompt in the item.</p> </li> <li> <code>answer_key</code>               (<code>str</code>)           \u2013            <p>Key to store the response.</p> </li> </ul> Source code in <code>ragfit/processing/local_steps/api/openai.py</code> <pre><code>def __init__(self, model, instruction, prompt_key, answer_key, **kwargs):\n    \"\"\"\n    Args:\n        model (dict): Configuration for the OpenAIExecutor.\n        instruction (str): Path to the system instruction file.\n        prompt_key (str): Key to the prompt in the item.\n        answer_key (str): Key to store the response.\n    \"\"\"\n    super().__init__(**kwargs)\n\n    self.model = OpenAIExecutor(**model)\n    self.prompt_key = prompt_key\n    self.answer_key = answer_key\n    self.instruction = open(instruction).read()\n</code></pre>"},{"location":"reference/processing/local_steps/retrievers/haystack/","title":"Haystack","text":""},{"location":"reference/processing/local_steps/retrievers/haystack/#ragfit.processing.local_steps.retrievers.haystack.HaystackRetriever","title":"<code>HaystackRetriever</code>","text":"<p>               Bases: <code>LocalStep</code></p> <p>Class for document retrieval using Haystack v2 pipelines.</p> Source code in <code>ragfit/processing/local_steps/retrievers/haystack.py</code> <pre><code>class HaystackRetriever(LocalStep):\n    \"\"\"\n    Class for document retrieval using Haystack v2 pipelines.\n    \"\"\"\n\n    def __init__(self, pipeline_or_yaml_path, docs_key, query_key, **kwargs):\n        super().__init__(**kwargs)\n        from haystack import Pipeline\n\n        if isinstance(pipeline_or_yaml_path, str):\n            self.pipe = Pipeline.load(open(pipeline_or_yaml_path))\n        else:\n            self.pipe = pipeline_or_yaml_path\n\n        self.docs_key = docs_key\n        self.query_key = query_key\n\n    def default_query_function(self, query):\n        \"\"\"\n        Create the default querying of the pipeline, by inserting the input query into all mandatory fields.\n        \"\"\"\n        pipe_inputs = self.pipe.inputs()\n        query_dict = {}\n        for inp_node_name, inp_node_params in pipe_inputs.items():\n            for param_name, param_values in inp_node_params.items():\n                if param_values[\"is_mandatory\"]:\n                    if inp_node_name not in query_dict:\n                        query_dict[inp_node_name] = {}\n\n                    query_dict[inp_node_name][param_name] = query\n\n        return query_dict\n\n    def query(self, query, structure=None):\n        \"\"\"\n        Haystack v2 pipelines can have multiple inputs; structure specify how to call `pipe.run`.\n\n        For example, structure could look like this:\n        {\n            \"Retriever\": {\"query\": \"query\",},\n            \"Reranker\": {\"query\": \"query\"},\n        }\n        and we replace the **value** of each key with the query.\n        \"\"\"\n\n        if structure is None:\n            structure = self.default_query_function(query)\n        else:\n            for key, value in structure.items():\n                structure[key] = {k: query for k in value.keys()}\n\n        response = self.pipe.run(structure)\n        all_documents = []\n        for v in response.values():\n            if \"documents\" in v:\n                # has documents, add to list\n                all_documents += v[\"documents\"]\n\n        all_documents = [\n            {\"content\": d.content, \"title\": d.meta.get(\"title\")} for d in all_documents\n        ]\n\n        return all_documents\n\n    def process_item(self, item, index, datasets, **kwargs):\n        \"\"\"\n        Query the `query_key` in the item and store the results in the `docs_key`.\n        Retrieved documents are stored as a list of dictionaries with keys `content` and `title`.\n        \"\"\"\n        item[self.docs_key] = self.query(item[self.query_key])\n        return item\n</code></pre>"},{"location":"reference/processing/local_steps/retrievers/haystack/#ragfit.processing.local_steps.retrievers.haystack.HaystackRetriever.default_query_function","title":"<code>default_query_function(query)</code>","text":"<p>Create the default querying of the pipeline, by inserting the input query into all mandatory fields.</p> Source code in <code>ragfit/processing/local_steps/retrievers/haystack.py</code> <pre><code>def default_query_function(self, query):\n    \"\"\"\n    Create the default querying of the pipeline, by inserting the input query into all mandatory fields.\n    \"\"\"\n    pipe_inputs = self.pipe.inputs()\n    query_dict = {}\n    for inp_node_name, inp_node_params in pipe_inputs.items():\n        for param_name, param_values in inp_node_params.items():\n            if param_values[\"is_mandatory\"]:\n                if inp_node_name not in query_dict:\n                    query_dict[inp_node_name] = {}\n\n                query_dict[inp_node_name][param_name] = query\n\n    return query_dict\n</code></pre>"},{"location":"reference/processing/local_steps/retrievers/haystack/#ragfit.processing.local_steps.retrievers.haystack.HaystackRetriever.process_item","title":"<code>process_item(item, index, datasets, **kwargs)</code>","text":"<p>Query the <code>query_key</code> in the item and store the results in the <code>docs_key</code>. Retrieved documents are stored as a list of dictionaries with keys <code>content</code> and <code>title</code>.</p> Source code in <code>ragfit/processing/local_steps/retrievers/haystack.py</code> <pre><code>def process_item(self, item, index, datasets, **kwargs):\n    \"\"\"\n    Query the `query_key` in the item and store the results in the `docs_key`.\n    Retrieved documents are stored as a list of dictionaries with keys `content` and `title`.\n    \"\"\"\n    item[self.docs_key] = self.query(item[self.query_key])\n    return item\n</code></pre>"},{"location":"reference/processing/local_steps/retrievers/haystack/#ragfit.processing.local_steps.retrievers.haystack.HaystackRetriever.query","title":"<code>query(query, structure=None)</code>","text":"<p>Haystack v2 pipelines can have multiple inputs; structure specify how to call <code>pipe.run</code>.</p> <p>For example, structure could look like this: {     \"Retriever\": {\"query\": \"query\",},     \"Reranker\": {\"query\": \"query\"}, } and we replace the value of each key with the query.</p> Source code in <code>ragfit/processing/local_steps/retrievers/haystack.py</code> <pre><code>def query(self, query, structure=None):\n    \"\"\"\n    Haystack v2 pipelines can have multiple inputs; structure specify how to call `pipe.run`.\n\n    For example, structure could look like this:\n    {\n        \"Retriever\": {\"query\": \"query\",},\n        \"Reranker\": {\"query\": \"query\"},\n    }\n    and we replace the **value** of each key with the query.\n    \"\"\"\n\n    if structure is None:\n        structure = self.default_query_function(query)\n    else:\n        for key, value in structure.items():\n            structure[key] = {k: query for k in value.keys()}\n\n    response = self.pipe.run(structure)\n    all_documents = []\n    for v in response.values():\n        if \"documents\" in v:\n            # has documents, add to list\n            all_documents += v[\"documents\"]\n\n    all_documents = [\n        {\"content\": d.content, \"title\": d.meta.get(\"title\")} for d in all_documents\n    ]\n\n    return all_documents\n</code></pre>"}]}